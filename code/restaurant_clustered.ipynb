{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cihaz Kümeleme için Veri Ön İşleme\n",
    "\n",
    "Bu notebook, `mobil_restaurant.csv` dosyasındaki veriyi cihaz bazlı kümeleme için hazırlar.\n",
    "\n",
    "Adımlar:\n",
    "1. Google Drive'ı bağlama.\n",
    "2. Gerekli kütüphaneleri yükleme.\n",
    "3. Veriyi Google Drive'dan yükleme (timestamp dahil).\n",
    "4. Özellik dönüşümlerini uygulama (Aşama 2) (timestamp dahil).\n",
    "5. Dönüştürülmüş veriyi kaydetme (`transformed_visits.csv`).\n",
    "6. Cihaz bazlı özellik mühendisliği (Aşama 3) (zaman dilimi özelliklerini ekleyerek).\n",
    "7. Cihaz bazlı özellikleri kaydetme (`device_features.csv`).\n",
    "8. Özellik filtreleme (düşük varyans, yüksek korelasyon).\n",
    "9. Özellik ölçeklendirme.\n",
    "10. Optimal küme sayısını belirleme ve K-Means kümeleme uygulama.\n",
    "11. Kümelenmiş veriyi kaydetme (`device_clusters.csv`).\n",
    "12. Küme personaları için görsel analizler (grafikleri Drive'a kaydetme dahil).\n",
    "13. Küme Bazlı Müşteri Çeşidi Analizi (EKLENDİ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Drive'ı Bağlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gerekli Kütüphaneleri Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Veriyi Google Drive'dan Yükleme\n",
    "\n",
    "Lütfen `file_path_on_drive` değişkenini CSV dosyanızın Google Drive'daki doğru yoluyla güncelleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_on_drive = '/content/drive/MyDrive/mobil_restaurant.csv'  # BU YOLU GÜNCELLEYİN!\n",
    "\n",
    "columns_to_load = [\n",
    "    'device_aid',\n",
    "    'SatisHacmi',\n",
    "    'BiletEtkinlik',\n",
    "    'OrtalamaHarcamaTutari',\n",
    "    'Mapin Segment',\n",
    "    'timestamp'  # Zaman damgası sütununu ekledik\n",
    "]\n",
    "\n",
    "print(f\"Loading {file_path_on_drive}...\")\n",
    "try:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path_on_drive, usecols=columns_to_load, sep=';', low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 decoding failed, trying latin1...\")\n",
    "        df = pd.read_csv(file_path_on_drive, usecols=columns_to_load, sep=';', low_memory=False, encoding='latin1')\n",
    "    print(\"CSV loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {file_path_on_drive} was not found. Please check the path.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Initial data sample:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Özellik Dönüşümlerini Uygulama (Aşama 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is None:\n",
    "    print(\"DataFrame 'df' not loaded. Cannot proceed with transformations.\")\n",
    "else:\n",
    "    # 1. SatisHacmi_Numeric\n",
    "    satis_hacmi_mapping = {'S1': 0, 'S2': 1, 'S3': 2}\n",
    "    df['SatisHacmi_Numeric'] = df['SatisHacmi'].map(satis_hacmi_mapping).fillna(-1)\n",
    "    print(\"SatisHacmi_Numeric created.\")\n",
    "\n",
    "    # 2. BiletEtkinlik_Numeric\n",
    "    bilet_etkinlik_mapping = {'Var': 1, 'Yok': 0}\n",
    "    df['BiletEtkinlik_Numeric'] = df['BiletEtkinlik'].map(bilet_etkinlik_mapping).fillna(-1)\n",
    "    print(\"BiletEtkinlik_Numeric created.\")\n",
    "\n",
    "    # 3. OrtalamaHarcamaTutari_Numeric\n",
    "    harcama_tutari_mapping = {\n",
    "        '0-499 TL': 1, '500-999 TL': 2, '1000-1.999 TL': 3, '2.000+TL': 4\n",
    "    }\n",
    "    df['OrtalamaHarcamaTutari_Cleaned'] = df['OrtalamaHarcamaTutari'].fillna('Boş').astype(str).str.strip()\n",
    "    df['OrtalamaHarcamaTutari_Numeric'] = df['OrtalamaHarcamaTutari_Cleaned'].map(harcama_tutari_mapping).fillna(0)\n",
    "    print(\"OrtalamaHarcamaTutari_Numeric created.\")\n",
    "\n",
    "    # 4. Mapin Segment Ayrıştırması\n",
    "    def parse_mapin_segment(segment_str):\n",
    "        if pd.isna(segment_str) or not isinstance(segment_str, str) or len(segment_str) < 2:\n",
    "            return pd.NA, pd.NA, pd.NA\n",
    "        venue_type = segment_str[0] if segment_str[0] in ['D', 'R', 'H'] else pd.NA\n",
    "        pop_match = re.search(r'[DRH](\\d)', segment_str)\n",
    "        population_score = int(pop_match.group(1)) if pop_match and 1 <= int(pop_match.group(1)) <= 5 else pd.NA\n",
    "        quality_char = segment_str.split('-')[-1] if '-' in segment_str and len(segment_str.split('-')[-1]) == 1 else pd.NA\n",
    "        quality_mapping = {'A': 2, 'B': 1, 'C': 0}\n",
    "        quality_score_numeric = quality_mapping.get(quality_char, pd.NA)\n",
    "        return venue_type, population_score, quality_score_numeric\n",
    "    \n",
    "    parsed_segments = df['Mapin Segment'].apply(parse_mapin_segment)\n",
    "    df['VenueType_Parsed'] = parsed_segments.apply(lambda x: x[0] if isinstance(x, tuple) else pd.NA)\n",
    "    df['PopulationInverseScore_Parsed'] = parsed_segments.apply(lambda x: x[1] if isinstance(x, tuple) else pd.NA)\n",
    "    df['QualityScore_Numeric'] = parsed_segments.apply(lambda x: x[2] if isinstance(x, tuple) else pd.NA)\n",
    "    print(\"Mapin Segment parsed.\")\n",
    "\n",
    "    # Timestamp'i olduğu gibi bırakıyoruz, sonraki aşamada işlenecek\n",
    "    df['timestamp_processed'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')\n",
    "    print(\"Timestamp converted to datetime.\")\n",
    "\n",
    "    output_columns = [\n",
    "        'device_aid',\n",
    "        'SatisHacmi_Numeric',\n",
    "        'BiletEtkinlik_Numeric',\n",
    "        'OrtalamaHarcamaTutari_Numeric',\n",
    "        'VenueType_Parsed',\n",
    "        'PopulationInverseScore_Parsed',\n",
    "        'QualityScore_Numeric',\n",
    "        'timestamp_processed' # İşlenmiş timestamp'i ekledik\n",
    "    ]\n",
    "    df_transformed = df[output_columns].copy()\n",
    "    df_transformed.dropna(subset=['timestamp_processed'], inplace=True) # Hatalı timestamp olan satırları çıkar\n",
    "\n",
    "    print(\"\\nTransformed data sample:\")\n",
    "    print(df_transformed.head().to_string())\n",
    "    print(\"\\nTransformed data info:\")\n",
    "    df_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dönüştürülmüş Veriyi Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_transformed' in locals() and df_transformed is not None:\n",
    "    output_csv_path = 'transformed_visits.csv'\n",
    "    print(f\"Saving transformed data to {output_csv_path}...\")\n",
    "    try:\n",
    "        df_transformed.to_csv(output_csv_path, index=False, sep=';')\n",
    "        print(f\"Transformed data saved successfully to {output_csv_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {e}\")\n",
    "else:\n",
    "    print(\"df_transformed DataFrame not found or is None. Cannot save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cihaz Bazlı Özellik Mühendisliği (Aşama 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_visits_path = 'transformed_visits.csv'\n",
    "print(f\"Loading {transformed_visits_path} for device-level aggregation...\")\n",
    "try:\n",
    "    df_visits = pd.read_csv(transformed_visits_path, sep=';')\n",
    "    df_visits['timestamp_processed'] = pd.to_datetime(df_visits['timestamp_processed'], errors='coerce')\n",
    "    df_visits.dropna(subset=['timestamp_processed'], inplace=True)\n",
    "    print(\"Transformed visits data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {transformed_visits_path} not found.\")\n",
    "    df_visits = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {transformed_visits_path}: {e}\")\n",
    "    df_visits = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_visits is not None:\n",
    "    print(\"Aggregating features per device_aid...\")\n",
    "\n",
    "    # Zaman Dilimi Özellikleri\n",
    "    df_visits['visit_hour'] = df_visits['timestamp_processed'].dt.hour\n",
    "    \n",
    "    def get_time_slot(hour):\n",
    "        if 6 <= hour <= 11: return 'Morning'    # 06:00 - 11:59\n",
    "        elif 12 <= hour <= 17: return 'Afternoon' # 12:00 - 17:59\n",
    "        elif 18 <= hour <= 23: return 'Evening'   # 18:00 - 23:59\n",
    "        else: return 'Night'                      # 00:00 - 05:59\n",
    "        \n",
    "    df_visits['TimeSlot'] = df_visits['visit_hour'].apply(get_time_slot)\n",
    "    \n",
    "    time_slot_dummies = pd.get_dummies(df_visits['TimeSlot'], prefix='TimeSlot')\n",
    "    df_visits_with_time_dummies = pd.concat([df_visits[['device_aid']], time_slot_dummies], axis=1)\n",
    "    df_time_slot_counts = df_visits_with_time_dummies.groupby('device_aid').sum()\n",
    "\n",
    "    # Ana Agregasyonlar\n",
    "    aggregation_functions = {\n",
    "        'SatisHacmi_Numeric': 'mean',\n",
    "        'BiletEtkinlik_Numeric': 'mean',\n",
    "        'OrtalamaHarcamaTutari_Numeric': 'mean',\n",
    "        'PopulationInverseScore_Parsed': 'mean',\n",
    "        'QualityScore_Numeric': 'mean',\n",
    "        'device_aid': 'count'\n",
    "    }\n",
    "    df_device_features = df_visits.groupby('device_aid').agg(aggregation_functions)\n",
    "    df_device_features.rename(columns={'device_aid': 'total_visits'}, inplace=True)\n",
    "\n",
    "    # VenueType Oranları\n",
    "    venue_type_dummies = pd.get_dummies(df_visits['VenueType_Parsed'], prefix='VenueType')\n",
    "    df_visits_with_venue_dummies = pd.concat([df_visits['device_aid'], venue_type_dummies], axis=1)\n",
    "    df_venue_type_counts = df_visits_with_venue_dummies.groupby('device_aid').sum()\n",
    "    df_device_features = df_device_features.join(df_venue_type_counts)\n",
    "    \n",
    "    # Zaman Dilimi Oranlarını Ekleme\n",
    "    df_device_features = df_device_features.join(df_time_slot_counts)\n",
    "\n",
    "    # Oranları Hesaplama (VenueType ve TimeSlot için)\n",
    "    rate_cols_to_create = list(venue_type_dummies.columns) + list(time_slot_dummies.columns)\n",
    "    for col_name in rate_cols_to_create:\n",
    "        if col_name in df_device_features.columns and 'total_visits' in df_device_features.columns:\n",
    "            df_device_features[col_name + '_rate'] = df_device_features.apply(\n",
    "                lambda row: row[col_name] / row['total_visits'] if row['total_visits'] > 0 else 0, axis=1\n",
    "            )\n",
    "            df_device_features.drop(columns=[col_name], inplace=True)\n",
    "        elif col_name not in df_device_features.columns: # Eğer bir dummy hiç oluşmadıysa (örn: hiç Night ziyareti yoksa)\n",
    "             df_device_features[col_name + '_rate'] = 0\n",
    "\n",
    "    df_device_features.rename(columns={\n",
    "        'SatisHacmi_Numeric': 'avg_SatisHacmi',\n",
    "        'BiletEtkinlik_Numeric': 'rate_BiletEtkinlik_Var',\n",
    "        'OrtalamaHarcamaTutari_Numeric': 'avg_OrtalamaHarcamaTutari',\n",
    "        'PopulationInverseScore_Parsed': 'avg_PopulationInverseScore',\n",
    "        'QualityScore_Numeric': 'avg_QualityScore'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    df_device_features = df_device_features.fillna(0)\n",
    "    \n",
    "    print(\"\\nDevice features aggregated (including time slots).\")\n",
    "    print(df_device_features.head().to_string())\n",
    "    print(\"\\nInfo for aggregated device features:\")\n",
    "    df_device_features.info()\n",
    "else:\n",
    "    print(\"df_visits DataFrame not found. Cannot aggregate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cihaz Bazlı Özellikleri Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_device_features' in locals() and df_device_features is not None:\n",
    "    device_features_path = 'device_features.csv'\n",
    "    print(f\"Saving device features to {device_features_path}...\")\n",
    "    try:\n",
    "        df_device_features.to_csv(device_features_path, sep=';')\n",
    "        print(f\"Device features saved successfully to {device_features_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving device features CSV: {e}\")\n",
    "else:\n",
    "    print(\"df_device_features DataFrame not found. Cannot save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Özellik Filtreleme ve Ölçeklendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794eba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_features_path = 'device_features.csv'\n",
    "print(f\"Loading {device_features_path} for filtering and scaling...\")\n",
    "try:\n",
    "    df_agg_features = pd.read_csv(device_features_path, sep=';', index_col='device_aid')\n",
    "    print(\"Aggregated device features loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {device_features_path} not found.\")\n",
    "    df_agg_features = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {device_features_path}: {e}\")\n",
    "    df_agg_features = None\n",
    "\n",
    "if df_agg_features is not None:\n",
    "    print(\"\\nData types before filtering/scaling:\")\n",
    "    print(df_agg_features.dtypes)\n",
    "    df_features_to_filter = df_agg_features.copy()\n",
    "else:\n",
    "    print(\"Skipping feature filtering/scaling as df_agg_features was not loaded.\")\n",
    "    df_features_to_filter = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb47df8",
   "metadata": {},
   "source": [
    "### 8.1. Düşük Varyanslı Özelliklerin Çıkarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_features_to_filter is not None:\n",
    "    print(\"\\nOriginal number of features:\", df_features_to_filter.shape[1])\n",
    "    variance_threshold_value = 0.01 \n",
    "    selector = VarianceThreshold(threshold=variance_threshold_value)\n",
    "    try:\n",
    "        selector.fit(df_features_to_filter)\n",
    "        retained_features_mask = selector.get_support()\n",
    "        removed_low_variance_features = df_features_to_filter.columns[~retained_features_mask]\n",
    "        df_agg_features = df_features_to_filter.loc[:, retained_features_mask].copy()\n",
    "        if len(removed_low_variance_features) > 0:\n",
    "            print(f\"Removed {len(removed_low_variance_features)} low-variance features (threshold < {variance_threshold_value}):\")\n",
    "            for feature in removed_low_variance_features:\n",
    "                print(f\"- {feature} (Variance: {df_features_to_filter[feature].var():.4f})\")\n",
    "        else:\n",
    "            print(f\"No features removed by variance threshold (< {variance_threshold_value}).\")\n",
    "        print(f\"Number of features after variance filtering: {df_agg_features.shape[1]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during variance thresholding: {e}. Skipping.\")\n",
    "        df_agg_features = df_features_to_filter.copy()\n",
    "else:\n",
    "    print(\"Skipping variance thresholding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2670074",
   "metadata": {},
   "source": [
    "### 8.2. Yüksek Korelasyonlu Özelliklerin Çıkarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6721351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_agg_features' in locals() and df_agg_features is not None and df_agg_features.shape[1] > 1:\n",
    "    print(\"\\nStarting high-correlation feature removal...\")\n",
    "    corr_matrix = df_agg_features.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    correlation_threshold = 0.90\n",
    "    features_to_drop_corr = set()\n",
    "    removed_correlation_info = []\n",
    "    for column in upper_triangle.columns:\n",
    "        if column in features_to_drop_corr: continue\n",
    "        highly_correlated_with_column = upper_triangle[upper_triangle[column] > correlation_threshold].index\n",
    "        for feature in highly_correlated_with_column:\n",
    "            if feature not in features_to_drop_corr:\n",
    "                features_to_drop_corr.add(feature)\n",
    "                removed_correlation_info.append(f\"- '{feature}' (>{correlation_threshold*100:.0f}% vs '{column}': {corr_matrix.loc[column, feature]:.2f})\")\n",
    "    if len(features_to_drop_corr) > 0:\n",
    "        df_agg_features.drop(columns=list(features_to_drop_corr), inplace=True)\n",
    "        print(f\"Removed {len(features_to_drop_corr)} highly-correlated features (threshold > {correlation_threshold}):\")\n",
    "        for info in removed_correlation_info:\n",
    "            print(info)\n",
    "    else:\n",
    "        print(f\"No features removed by correlation threshold (> {correlation_threshold}).\")\n",
    "    print(f\"Number of features after correlation filtering: {df_agg_features.shape[1]}\")\n",
    "elif 'df_agg_features' in locals() and df_agg_features is not None:\n",
    "    print(\"Skipping correlation filtering (1 or no features left).\")\n",
    "else:\n",
    "    print(\"Skipping correlation filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14ea78",
   "metadata": {},
   "source": [
    "### 8.3. Ölçeklendirme (Filtrelenmiş Özelliklerle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c207ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_agg_features' in locals() and df_agg_features is not None and df_agg_features.shape[1] > 0:\n",
    "    print(\"\\nScaling the remaining features...\")\n",
    "    feature_columns = df_agg_features.columns\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled_features_array = scaler.fit_transform(df_agg_features[feature_columns])\n",
    "    df_scaled_features = pd.DataFrame(df_scaled_features_array, columns=feature_columns, index=df_agg_features.index)\n",
    "    print(\"Features scaled successfully.\")\n",
    "    print(df_scaled_features.head().to_string())\n",
    "    print(f\"Shape of scaled features: {df_scaled_features.shape}\")\n",
    "elif 'df_agg_features' in locals() and df_agg_features is not None:\n",
    "    print(\"No features remaining after filtering. Clustering cannot proceed.\")\n",
    "    df_scaled_features = None\n",
    "else:\n",
    "    print(\"Skipping scaling.\")\n",
    "    df_scaled_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Aşama 5: Kümeleme Algoritmasının Uygulanması (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_scaled_features' in locals() and df_scaled_features is not None and not df_scaled_features.empty:\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 11)\n",
    "    print(\"\\nCalculating inertia and silhouette scores for K range...\")\n",
    "    for k_val in k_range:\n",
    "        kmeans = KMeans(n_clusters=k_val, random_state=42, n_init='auto')\n",
    "        kmeans.fit(df_scaled_features)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        score = silhouette_score(df_scaled_features, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"K={k_val}, Inertia: {kmeans.inertia_:.2f}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_range, inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal K')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.xticks(k_range)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(k_range, silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Scores for Optimal K')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(k_range)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if silhouette_scores:\n",
    "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"\\nOptimal K based on highest Silhouette Score: {optimal_k}\")\n",
    "    else:\n",
    "        print(\"Could not determine optimal K. Defaulting to K=3.\")\n",
    "        optimal_k = 3\n",
    "else:\n",
    "    print(\"\\nSkipping K selection as scaled features are not available or empty.\")\n",
    "    optimal_k = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. K-Means Kümelemesini Uygulama (Optimal K ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_scaled_features' in locals() and df_scaled_features is not None and not df_scaled_features.empty and optimal_k is not None:\n",
    "    print(f\"\\nApplying K-Means with K={optimal_k}...\")\n",
    "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans_final.fit_predict(df_scaled_features)\n",
    "    \n",
    "    df_clustered_devices = df_agg_features.copy() # Use features before scaling for easier interpretation\n",
    "    df_clustered_devices['cluster'] = cluster_labels\n",
    "    \n",
    "    print(\"K-Means clustering applied. Cluster labels added.\")\n",
    "    print(df_clustered_devices.head().to_string())\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    print(df_clustered_devices['cluster'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"\\nSkipping K-Means application.\")\n",
    "    df_clustered_devices = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Kümelenmiş Veriyi Kaydetme ve Küme Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clustered_devices' in locals() and df_clustered_devices is not None:\n",
    "    clustered_output_path = 'device_clusters.csv'\n",
    "    print(f\"\\nSaving clustered device data to {clustered_output_path}...\")\n",
    "    try:\n",
    "        df_clustered_devices.to_csv(clustered_output_path, sep=';')\n",
    "        print(f\"Clustered device data saved successfully to {clustered_output_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving clustered data CSV: {e}\")\n",
    "    \n",
    "    print(\"\\nCluster characteristics (mean values of features per cluster):\")\n",
    "    cluster_analysis = df_clustered_devices.groupby('cluster').mean()\n",
    "    print(cluster_analysis.to_string())\n",
    "else:\n",
    "    print(\"\\ndf_clustered_devices DataFrame not found. Skipping save and analysis.\")\n",
    "    cluster_analysis = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1. Küme Personaları için Görsel Analizler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster_analysis' in locals() and cluster_analysis is not None and 'df_clustered_devices' in locals() and df_clustered_devices is not None:\n",
    "    print(\"\\nGenerating visualizations for cluster personas...\")\n",
    "    \n",
    "    drive_save_path_base = '/content/drive/MyDrive/restaurant_clustering_results'\n",
    "    if not os.path.exists(drive_save_path_base):\n",
    "        os.makedirs(drive_save_path_base)\n",
    "        print(f\"Created directory: {drive_save_path_base}\")\n",
    "    \n",
    "    features_for_plotting = cluster_analysis.columns.tolist()\n",
    "    num_clusters = len(cluster_analysis)\n",
    "\n",
    "    # Radar Charts (using scaled cluster centers from kmeans_final)\n",
    "    if 'kmeans_final' in locals() and hasattr(kmeans_final, 'cluster_centers_') and 'df_scaled_features' in locals() and df_scaled_features is not None:\n",
    "        scaled_cluster_centers = kmeans_final.cluster_centers_\n",
    "        feature_names_for_radar = df_scaled_features.columns.tolist()\n",
    "        angles = np.linspace(0, 2 * np.pi, len(feature_names_for_radar), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "        for i in range(num_clusters):\n",
    "            values = scaled_cluster_centers[i].tolist()\n",
    "            values += values[:1]\n",
    "            ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Cluster {i}\")\n",
    "            ax.fill(angles, values, alpha=0.25)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(feature_names_for_radar, size=8)\n",
    "        ax.set_title(\"Radar Chart of Scaled Cluster Centers\", size=16, y=1.1)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        radar_chart_path = os.path.join(drive_save_path_base, 'radar_chart_cluster_personas.png')\n",
    "        try: plt.savefig(radar_chart_path, bbox_inches='tight'); print(f\"Radar chart saved to: {radar_chart_path}\")\n",
    "        except Exception as e: print(f\"Error saving radar chart: {e}\")\n",
    "        plt.show()\n",
    "    else: print(\"Skipping radar chart: prerequisites not met.\")\n",
    "\n",
    "    # Box Plots (using unscaled data in df_clustered_devices)\n",
    "    if features_for_plotting and not df_clustered_devices.empty:\n",
    "        print(\"\\nGenerating box plots...\")\n",
    "        num_features_to_plot = len(features_for_plotting)\n",
    "        cols_subplot = 3\n",
    "        rows_subplot = (num_features_to_plot + cols_subplot - 1) // cols_subplot\n",
    "        plt.figure(figsize=(15, rows_subplot * 5))\n",
    "        for i, feature in enumerate(features_for_plotting):\n",
    "            plt.subplot(rows_subplot, cols_subplot, i + 1)\n",
    "            sns.boxplot(x='cluster', y=feature, data=df_clustered_devices, palette='viridis')\n",
    "            plt.title(f'{feature} by Cluster', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        box_plots_path = os.path.join(drive_save_path_base, 'box_plots_feature_distribution.png')\n",
    "        try: plt.savefig(box_plots_path, bbox_inches='tight'); print(f\"Box plots saved to: {box_plots_path}\")\n",
    "        except Exception as e: print(f\"Error saving box plots: {e}\")\n",
    "        plt.show()\n",
    "    else: print(\"Skipping box plots: no features or data.\")\n",
    "\n",
    "    # Bar charts of mean feature values (from cluster_analysis)\n",
    "    if not cluster_analysis.empty:\n",
    "        print(\"\\nGenerating bar chart of mean features...\")\n",
    "        cluster_analysis.T.plot(kind='bar', figsize=(15, 8), colormap='viridis')\n",
    "        plt.title('Mean Feature Values by Cluster (Original Scale)')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Cluster')\n",
    "        plt.tight_layout()\n",
    "        bar_chart_path = os.path.join(drive_save_path_base, 'bar_chart_mean_features.png')\n",
    "        try: plt.savefig(bar_chart_path, bbox_inches='tight'); print(f\"Bar chart saved to: {bar_chart_path}\")\n",
    "        except Exception as e: print(f\"Error saving bar chart: {e}\")\n",
    "        plt.show()\n",
    "    else: print(\"Skipping bar chart: cluster_analysis is empty.\")\n",
    "else:\n",
    "    print(\"\\nSkipping persona visualizations: prerequisites not met.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "**Sonraki Adımlar:**\n",
    "1. Bu notebook'u çalıştırın ve `file_path_on_drive` değişkenini güncellediğinizden emin olun.\n",
    "2. Tüm adımların (veri yükleme, dönüşüm, özellik mühendisliği, filtreleme, ölçeklendirme, kümeleme, görselleştirme) çıktılarını inceleyin.\n",
    "3. Özellikle, hangi özelliklerin çıkarıldığını, optimal K seçimini ve küme personalarını gösteren grafikleri değerlendirin.\n",
    "4. `device_clusters.csv` dosyası ve Google Drive'a kaydedilen grafikler nihai çıktılarınızdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Küme Bazlı Müşteri Çeşidi Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting post-clustering analysis for MusteriCesidi...\")\n",
    "\n",
    "# Gerekli dosyaların yolları\n",
    "clustered_devices_filepath = 'device_clusters.csv' # Kümelenmiş cihaz verisi\n",
    "# mobil_restaurant.csv dosyasının yolu daha önce 'file_path_on_drive' olarak tanımlanmıştı.\n",
    "# Eğer bu hücre ayrı çalıştırılıyorsa veya kapsam dışı kaldıysa, yeniden tanımlamak gerekebilir.\n",
    "# Şimdilik 'file_path_on_drive' değişkeninin hala geçerli olduğunu varsayıyoruz.\n",
    "original_data_filepath = file_path_on_drive \n",
    "\n",
    "try:\n",
    "    # Kümelenmiş cihaz verisini yükle\n",
    "    print(f\"Loading clustered device data from {clustered_devices_filepath}...\")\n",
    "    df_clusters = pd.read_csv(clustered_devices_filepath, sep=';')\n",
    "    print(f\"Clustered data loaded. Shape: {df_clusters.shape}\")\n",
    "    if 'device_aid' not in df_clusters.columns or 'cluster' not in df_clusters.columns:\n",
    "        print(\"Error: 'device_aid' or 'cluster' column missing in device_clusters.csv\")\n",
    "        raise SystemExit(\"Exiting due to missing columns in cluster data.\")\n",
    "\n",
    "    # Orijinal mobil_restaurant verisinden sadece device_aid ve MusteriCesidi sütunlarını yükle\n",
    "    print(f\"Loading MusteriCesidi data from {original_data_filepath}...\")\n",
    "    try:\n",
    "        df_musteri_cesidi = pd.read_csv(original_data_filepath, sep=';', usecols=['device_aid', 'MusteriCesidi'], low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 decoding failed for mobil_restaurant.csv, trying latin1...\")\n",
    "        df_musteri_cesidi = pd.read_csv(original_data_filepath, sep=';', usecols=['device_aid', 'MusteriCesidi'], low_memory=False, encoding='latin1')\n",
    "    print(f\"MusteriCesidi data loaded. Shape: {df_musteri_cesidi.shape}\")\n",
    "    if 'MusteriCesidi' not in df_musteri_cesidi.columns:\n",
    "        print(\"Error: 'MusteriCesidi' column missing in mobil_restaurant.csv\")\n",
    "        raise SystemExit(\"Exiting due to missing MusteriCesidi column.\")\n",
    "\n",
    "    # İki DataFrame'i device_aid üzerinden birleştir\n",
    "    print(\"Merging clustered data with MusteriCesidi data...\")\n",
    "    # df_clusters'daki device_aid'ler benzersiz olmalı (index_col='device_aid' ile okunmuştu device_features kaydedilirken)\n",
    "    # df_musteri_cesidi'de device_aid başına birden fazla satır olabilir (her ziyaret için bir satır)\n",
    "    # Bu durumda, her cihaz için MusteriCesidi'nin nasıl ele alınacağına karar vermek gerekir.\n",
    "    # Eğer mobil_restaurant.csv'de device_aid başına MusteriCesidi tekilse sorun yok.\n",
    "    # Eğer değilse, her device_aid için bir MusteriCesidi seçilmeli (örn: ilk görülen, en sık görülen)\n",
    "    # Şimdilik, mobil_restaurant.csv'deki her device_aid için MusteriCesidi'nin tutarlı olduğunu varsayalım\n",
    "    # ve birleştirmeden önce df_musteri_cesidi'yi tekilleştirelim.\n",
    "    df_musteri_cesidi_unique = df_musteri_cesidi.drop_duplicates(subset=['device_aid']).copy()\n",
    "    \n",
    "    df_merged_analysis = pd.merge(df_clusters[['device_aid', 'cluster']], df_musteri_cesidi_unique, on='device_aid', how='left')\n",
    "    print(f\"Merge completed. Shape of merged data for analysis: {df_merged_analysis.shape}\")\n",
    "\n",
    "    if df_merged_analysis.empty:\n",
    "        print(\"Merged data for analysis is empty. Cannot proceed.\")\n",
    "    else:\n",
    "        print(\"\\n--- Küme Bazlı Müşteri Çeşidi Dağılımı ---\")\n",
    "        # Her küme için MusteriCesidi sayımlarını al\n",
    "        musteri_cesidi_distribution = df_merged_analysis.groupby('cluster')['MusteriCesidi'].value_counts(normalize=False).unstack(fill_value=0)\n",
    "        print(\"\\nMüşteri Çeşidi Sayıları Her Küme İçin:\")\n",
    "        print(musteri_cesidi_distribution.to_string())\n",
    "\n",
    "        # Her küme için en yaygın MusteriCesidi (mode)\n",
    "        print(\"\\nHer Küme İçin En Yaygın Müşteri Çeşidi (Mod):\")\n",
    "        # mode() bir Series döndürür, birden fazla mod olabilir, ilkini alıyoruz.\n",
    "        cluster_modes = df_merged_analysis.groupby('cluster')['MusteriCesidi'].apply(lambda x: x.mode()[0] if not x.mode().empty else 'N/A')\n",
    "        print(cluster_modes.to_string())\n",
    "        \n",
    "        # İsteğe bağlı: Yüzdesel dağılım\n",
    "        musteri_cesidi_percentage = df_merged_analysis.groupby('cluster')['MusteriCesidi'].value_counts(normalize=True).mul(100).round(2).unstack(fill_value=0)\n",
    "        print(\"\\nMüşteri Çeşidi Yüzdeleri Her Küme İçin (%):\")\n",
    "        print(musteri_cesidi_percentage.to_string())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: One of the required files not found. {e}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column was not found in one of the DataFrames. {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during post-clustering analysis: {e}\")\n",
    "\n",
    "print(\"Post-clustering analysis for MusteriCesidi finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
