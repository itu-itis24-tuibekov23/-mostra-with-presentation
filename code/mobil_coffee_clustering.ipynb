{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobil Coffee Clustering\n",
    "\n",
    "This notebook performs clustering analysis on `mobil-coffee.csv` to identify customer segments based on their coffee shop visiting patterns, focusing on time spent in cafes and visit frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Load Data and Initial Processing (from Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# IMPORTANT: Update this path to the correct location of your mobil_coffee.csv file on Google Drive!\n",
    "file_path = '/content/drive/MyDrive/path/to/your/mobil_coffee.csv'\n",
    "print(f\"Attempting to load main data from: {file_path}\")\n",
    "\n",
    "# Define columns to load and their potential dtypes to save memory\n",
    "columns_to_load = {\n",
    "    'device_aid': 'str',\n",
    "    'timestamp': 'str', # Load as string first, then parse\n",
    "    'isim': 'str', # Cafe name\n",
    "    'original_latitude': 'float32',\n",
    "    'original_longitude': 'float32'\n",
    "}\n",
    "\n",
    "print(f\"Loading {file_path}...\")\n",
    "try:\n",
    "    # Attempt to load in chunks if the file is very large, or load directly\n",
    "    # For now, direct load with specified dtypes and columns\n",
    "    df = pd.read_csv(\n",
    "        file_path, \n",
    "        sep=';', \n",
    "        usecols=columns_to_load.keys(), \n",
    "        dtype=columns_to_load,\n",
    "        low_memory=False\n",
    "    )\n",
    "    print(\"CSV loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {file_path} was not found. Please check the path.\")\n",
    "    df = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    print(\"Initial data sample:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nData info:\")\n",
    "    df.info()\n",
    "\n",
    "    # Convert timestamp to datetime objects\n",
    "    print(\"\\nConverting timestamp to datetime...\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    \n",
    "    # Drop rows where timestamp conversion failed\n",
    "    df.dropna(subset=['timestamp'], inplace=True)\n",
    "    \n",
    "    # Sort data for visit identification\n",
    "    print(\"Sorting data by device_aid, isim, and timestamp...\")\n",
    "    df.sort_values(['device_aid', 'isim', 'timestamp'], inplace=True)\n",
    "    \n",
    "    print(\"\\nData after timestamp conversion and sorting:\")\n",
    "    print(df.head())\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify Individual Visits and Calculate Durations\n",
    "\n",
    "A visit is defined by consecutive pings from the same device at the same cafe. A new visit starts if the time gap between pings exceeds a threshold (e.g., 1 hour) or if the pings are on different days or at different cafe locations (though 'isim' might group different branches, using lat/lon could refine this if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Define a threshold for separating visits (e.g., 1 hour)\n",
    "    visit_separation_threshold = timedelta(hours=1)\n",
    "\n",
    "    # Add a 'cafe_location_id' to distinguish between different branches of the same cafe chain if 'isim' is too general\n",
    "    # For now, we can combine 'isim' with rounded lat/lon to create a more specific location ID\n",
    "    # Rounding lat/lon to ~100m precision (0.001 degrees)\n",
    "    df['cafe_location_id'] = df['isim'] + \"_\" + df['original_latitude'].round(3).astype(str) + \"_\" + df['original_longitude'].round(3).astype(str)\n",
    "    \n",
    "    print(\"Identifying visits...\")\n",
    "    # Calculate time difference between consecutive pings for the same device at the same cafe_location_id\n",
    "    df['time_diff_to_prev'] = df.groupby(['device_aid', 'cafe_location_id'])['timestamp'].diff()\n",
    "\n",
    "    # Identify the start of a new visit\n",
    "    # A new visit starts if:\n",
    "    # 1. It's the first ping for that device_aid at that cafe_location_id (time_diff_to_prev is NaT)\n",
    "    # 2. The time_diff_to_prev exceeds the visit_separation_threshold\n",
    "    # 3. The day changes (already implicitly handled by threshold if it's large enough, but can be explicit)\n",
    "    df['new_visit_marker'] = (\n",
    "        df['time_diff_to_prev'].isna() | \n",
    "        (df['time_diff_to_prev'] > visit_separation_threshold)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Create a visit_id by taking the cumulative sum of new_visit_marker for each device_aid at each cafe_location_id\n",
    "    df['visit_group_id'] = df.groupby(['device_aid', 'cafe_location_id'])['new_visit_marker'].cumsum()\n",
    "    df['visit_id'] = df['device_aid'] + \"_\" + df['cafe_location_id'] + \"_\" + df['visit_group_id'].astype(str)\n",
    "\n",
    "    print(\"Aggregating pings into visits...\")\n",
    "    # Aggregate pings into visits\n",
    "    visit_data = df.groupby('visit_id').agg(\n",
    "        device_aid=('device_aid', 'first'),\n",
    "        cafe_location_id=('cafe_location_id', 'first'),\n",
    "        isim=('isim', 'first'),\n",
    "        original_latitude=('original_latitude', 'first'), # Could use mean if pings vary slightly\n",
    "        original_longitude=('original_longitude', 'first'),\n",
    "        visit_start_time=('timestamp', 'min'),\n",
    "        visit_end_time=('timestamp', 'max'),\n",
    "        num_pings=('timestamp', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate visit duration\n",
    "    visit_data['visit_duration'] = visit_data['visit_end_time'] - visit_data['visit_start_time']\n",
    "\n",
    "    # Filter out very short durations if they are likely noise (e.g., less than 1 minute and only 1 ping)\n",
    "    # For now, let's keep all durations and analyze later.\n",
    "    # A duration of 0 is possible if there's only one ping for that visit.\n",
    "\n",
    "    # Extract hour and day of week from visit_start_time\n",
    "    visit_data['visit_start_hour'] = visit_data['visit_start_time'].dt.hour\n",
    "    visit_data['visit_start_day_of_week'] = visit_data['visit_start_time'].dt.day_name()\n",
    "\n",
    "    print(\"\\nSample of processed visit data:\")\n",
    "    print(visit_data.head().to_string())\n",
    "    print(\"\\nInfo for visit data:\")\n",
    "    visit_data.info()\n",
    "\n",
    "    # Save this intermediate result\n",
    "    # IMPORTANT: Update this path if you want to save to a different Drive location!\n",
    "    output_visits_path = '/content/drive/MyDrive/path/to/your/coffee_visits_detailed.csv'\n",
    "    print(f\"\\nSaving detailed visit data to {output_visits_path}...\")\n",
    "    try:\n",
    "        visit_data.to_csv(output_visits_path, index=False, sep=';')\n",
    "        print(f\"Detailed visit data saved successfully to {output_visits_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving detailed visit data CSV: {e}\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not loaded. Cannot proceed with visit identification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Device-Level Feature Aggregation\n",
    "\n",
    "Now, aggregate the visit data to create features for each unique device_aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Update this path to where 'coffee_visits_detailed.csv' was saved on Drive!\n",
    "detailed_visits_path = '/content/drive/MyDrive/path/to/your/coffee_visits_detailed.csv'\n",
    "print(f\"Loading {detailed_visits_path} for device-level aggregation...\")\n",
    "try:\n",
    "    df_visits = pd.read_csv(detailed_visits_path, sep=';')\n",
    "    # Ensure correct data types after loading, especially for datetime/timedelta\n",
    "    df_visits['visit_start_time'] = pd.to_datetime(df_visits['visit_start_time'])\n",
    "    df_visits['visit_end_time'] = pd.to_datetime(df_visits['visit_end_time'])\n",
    "    # Pandas read_csv doesn't automatically convert to timedelta; it might be a string.\n",
    "    # Convert visit_duration from string (e.g., \"0 days 01:23:45\") to timedelta seconds for easier aggregation.\n",
    "    df_visits['visit_duration_seconds'] = pd.to_timedelta(df_visits['visit_duration']).dt.total_seconds()\n",
    "    print(\"Detailed visits data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {detailed_visits_path} not found.\")\n",
    "    df_visits = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {detailed_visits_path}: {e}\")\n",
    "    df_visits = None\n",
    "\n",
    "if df_visits is not None:\n",
    "    print(\"Aggregating features per device_aid...\")\n",
    "\n",
    "    # Basic aggregations\n",
    "    device_features = df_visits.groupby('device_aid').agg(\n",
    "        total_visits=('visit_id', 'count'),\n",
    "        total_pings=('num_pings', 'sum'),\n",
    "        avg_visit_duration_seconds=('visit_duration_seconds', 'mean'),\n",
    "        total_time_spent_seconds=('visit_duration_seconds', 'sum'),\n",
    "        num_unique_cafes_visited=('cafe_location_id', 'nunique'),\n",
    "        first_visit_date=('visit_start_time', 'min'),\n",
    "        last_visit_date=('visit_start_time', 'max')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate overall observation period for frequency calculation\n",
    "    device_features['observation_period_days'] = (device_features['last_visit_date'] - device_features['first_visit_date']).dt.days + 1 # Add 1 to avoid division by zero if first=last\n",
    "    device_features['avg_visits_per_week'] = (device_features['total_visits'] / device_features['observation_period_days']) * 7\n",
    "    \n",
    "    # Time Slot Features (Morning, Afternoon, Evening, Night)\n",
    "    df_visits['visit_start_hour'] = df_visits['visit_start_time'].dt.hour # Ensure this column exists if not carried from previous cell\n",
    "    def get_time_slot(hour):\n",
    "        if 6 <= hour <= 11: return 'Morning'    # 06:00 - 11:59\n",
    "        elif 12 <= hour <= 17: return 'Afternoon' # 12:00 - 17:59\n",
    "        elif 18 <= hour <= 23: return 'Evening'   # 18:00 - 23:59\n",
    "        else: return 'Night'                      # 00:00 - 05:59\n",
    "    df_visits['TimeSlot'] = df_visits['visit_start_hour'].apply(get_time_slot)\n",
    "    time_slot_dummies = pd.get_dummies(df_visits.set_index('device_aid')['TimeSlot'], prefix='TimeSlot')\n",
    "    time_slot_counts = time_slot_dummies.groupby('device_aid').sum()\n",
    "    # Calculate rates\n",
    "    for col in time_slot_counts.columns:\n",
    "        time_slot_counts[col + '_rate'] = time_slot_counts[col] / device_features.set_index('device_aid')['total_visits']\n",
    "        time_slot_counts.drop(columns=[col], inplace=True)\n",
    "    device_features = device_features.merge(time_slot_counts, on='device_aid', how='left').fillna(0)\n",
    "\n",
    "    # Day of Week Features\n",
    "    df_visits['visit_start_day_of_week'] = df_visits['visit_start_time'].dt.day_name() # Ensure this column exists\n",
    "    day_of_week_dummies = pd.get_dummies(df_visits.set_index('device_aid')['visit_start_day_of_week'], prefix='DayOfWeek')\n",
    "    day_of_week_counts = day_of_week_dummies.groupby('device_aid').sum()\n",
    "    # Calculate rates\n",
    "    for col in day_of_week_counts.columns:\n",
    "        day_of_week_counts[col + '_rate'] = day_of_week_counts[col] / device_features.set_index('device_aid')['total_visits']\n",
    "        day_of_week_counts.drop(columns=[col], inplace=True)\n",
    "    device_features = device_features.merge(day_of_week_counts, on='device_aid', how='left').fillna(0)\n",
    "\n",
    "    # Convert avg_visit_duration to minutes for better interpretability\n",
    "    device_features['avg_visit_duration_minutes'] = device_features['avg_visit_duration_seconds'] / 60\n",
    "    device_features['total_time_spent_hours'] = device_features['total_time_spent_seconds'] / 3600\n",
    "    # Drop intermediate columns like seconds, dates used for calculation if not needed for clustering\n",
    "    device_features.drop(columns=['avg_visit_duration_seconds', 'total_time_spent_seconds', 'first_visit_date', 'last_visit_date', 'observation_period_days'], inplace=True, errors='ignore')\n",
    "\n",
    "    print(\"\\nSample of aggregated device features:\")\n",
    "    print(device_features.head().to_string())\n",
    "    print(\"\\nInfo for aggregated device features:\")\n",
    "    device_features.info()\n",
    "\n",
    "    # Save aggregated features\n",
    "    # IMPORTANT: Update this path if you want to save to a different Drive location!\n",
    "    output_agg_features_path = '/content/drive/MyDrive/path/to/your/coffee_device_aggregated_features.csv'\n",
    "    print(f\"\\nSaving aggregated device features to {output_agg_features_path}...\")\n",
    "    try:\n",
    "        device_features.to_csv(output_agg_features_path, index=False, sep=';')\n",
    "        print(f\"Aggregated device features saved successfully to {output_agg_features_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving aggregated device features CSV: {e}\")\n",
    "else:\n",
    "    print(\"df_visits DataFrame not found. Cannot aggregate device features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Preprocessing (Filtering and Scaling)\n",
    "\n",
    "Load the aggregated device features, filter out less informative ones, and scale them for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# IMPORTANT: Update this path to where 'coffee_device_aggregated_features.csv' was saved on Drive!\n",
    "agg_features_path = '/content/drive/MyDrive/path/to/your/coffee_device_aggregated_features.csv'\n",
    "print(f\"Loading {agg_features_path} for filtering and scaling...\")\n",
    "try:\n",
    "    df_agg_features = pd.read_csv(agg_features_path, sep=';')\n",
    "    # Set device_aid as index, as it's the identifier, not a feature for clustering\n",
    "    df_agg_features.set_index('device_aid', inplace=True)\n",
    "    print(\"Aggregated device features loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {agg_features_path} not found.\")\n",
    "    df_agg_features = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {agg_features_path}: {e}\")\n",
    "    df_agg_features = None\n",
    "\n",
    "if df_agg_features is not None:\n",
    "    print(\"\\nData types before filtering/scaling:\")\n",
    "    print(df_agg_features.dtypes)\n",
    "    df_features_to_filter = df_agg_features.copy()\n",
    "\n",
    "    # Ensure all columns are numeric for variance thresholding and correlation\n",
    "    # This should be the case already based on previous steps, but as a safeguard:\n",
    "    numeric_cols = df_features_to_filter.select_dtypes(include=np.number).columns\n",
    "    df_features_to_filter = df_features_to_filter[numeric_cols]\n",
    "    \n",
    "    # 8.1. Düşük Varyanslı Özelliklerin Çıkarılması (Copied from restaurant_clustered.ipynb and adapted)\n",
    "    print(\"\\nOriginal number of features:\", df_features_to_filter.shape[1])\n",
    "    variance_threshold_value = 0.01 # Adjust as needed\n",
    "    selector = VarianceThreshold(threshold=variance_threshold_value)\n",
    "    try:\n",
    "        # Ensure no NaN values before fitting VarianceThreshold\n",
    "        df_features_to_filter_no_nan = df_features_to_filter.fillna(0) # Or use another imputation strategy\n",
    "        selector.fit(df_features_to_filter_no_nan)\n",
    "        retained_features_mask = selector.get_support()\n",
    "        removed_low_variance_features = df_features_to_filter.columns[~retained_features_mask]\n",
    "        df_processed_features = df_features_to_filter.loc[:, retained_features_mask].copy()\n",
    "        if len(removed_low_variance_features) > 0:\n",
    "            print(f\"Removed {len(removed_low_variance_features)} low-variance features (threshold < {variance_threshold_value}):\")\n",
    "            for feature in removed_low_variance_features:\n",
    "                print(f\"- {feature} (Variance: {df_features_to_filter[feature].var():.4f})\")\n",
    "        else:\n",
    "            print(f\"No features removed by variance threshold (< {variance_threshold_value}).\")\n",
    "        print(f\"Number of features after variance filtering: {df_processed_features.shape[1]}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during variance thresholding: {e}. Skipping.\")\n",
    "        df_processed_features = df_features_to_filter.copy()\n",
    "\n",
    "    # 8.2. Yüksek Korelasyonlu Özelliklerin Çıkarılması (Copied and adapted)\n",
    "    if df_processed_features.shape[1] > 1:\n",
    "        print(\"\\nStarting high-correlation feature removal...\")\n",
    "        corr_matrix = df_processed_features.corr().abs()\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        correlation_threshold = 0.90 # Adjust as needed\n",
    "        features_to_drop_corr = set()\n",
    "        removed_correlation_info = []\n",
    "        for column in upper_triangle.columns:\n",
    "            if column in features_to_drop_corr: continue\n",
    "            highly_correlated_with_column = upper_triangle[upper_triangle[column] > correlation_threshold].index\n",
    "            for feature in highly_correlated_with_column:\n",
    "                if feature not in features_to_drop_corr:\n",
    "                    features_to_drop_corr.add(feature)\n",
    "                    removed_correlation_info.append(f\"- '{feature}' (>{correlation_threshold*100:.0f}% vs '{column}': {corr_matrix.loc[column, feature]:.2f})\")\n",
    "        if len(features_to_drop_corr) > 0:\n",
    "            df_processed_features.drop(columns=list(features_to_drop_corr), inplace=True)\n",
    "            print(f\"Removed {len(features_to_drop_corr)} highly-correlated features (threshold > {correlation_threshold}):\")\n",
    "            for info in removed_correlation_info:\n",
    "                print(info)\n",
    "        else:\n",
    "            print(f\"No features removed by correlation threshold (> {correlation_threshold}).\")\n",
    "        print(f\"Number of features after correlation filtering: {df_processed_features.shape[1]}\")\n",
    "    elif df_processed_features.shape[1] <= 1:\n",
    "        print(\"Skipping correlation filtering (1 or no features left).\")\n",
    "\n",
    "    # 8.3. Ölçeklendirme (Filtrelenmiş Özelliklerle)\n",
    "    if df_processed_features.shape[1] > 0:\n",
    "        print(\"\\nScaling the remaining features...\")\n",
    "        feature_columns_for_scaling = df_processed_features.columns\n",
    "        scaler = StandardScaler()\n",
    "        # Ensure no NaN values before scaling\n",
    "        df_processed_features_no_nan_scaling = df_processed_features.fillna(df_processed_features.mean()) # Impute with mean, or 0\n",
    "        df_scaled_features_array = scaler.fit_transform(df_processed_features_no_nan_scaling[feature_columns_for_scaling])\n",
    "        df_scaled_features = pd.DataFrame(df_scaled_features_array, columns=feature_columns_for_scaling, index=df_processed_features.index)\n",
    "        print(\"Features scaled successfully.\")\n",
    "        print(df_scaled_features.head().to_string())\n",
    "        print(f\"Shape of scaled features: {df_scaled_features.shape}\")\n",
    "    else:\n",
    "        print(\"No features remaining after filtering. Clustering cannot proceed.\")\n",
    "        df_scaled_features = None\n",
    "else:\n",
    "    print(\"Skipping feature filtering/scaling as df_agg_features was not loaded.\")\n",
    "    df_processed_features = None # To be used for K-Means input if scaling fails\n",
    "    df_scaled_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "if 'df_scaled_features' in locals() and df_scaled_features is not None and not df_scaled_features.empty:\n",
    "    inertia = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 11) # Test K from 2 to 10\n",
    "    print(\"\\nCalculating inertia and silhouette scores for K range...\")\n",
    "    for k_val in k_range:\n",
    "        kmeans = KMeans(n_clusters=k_val, random_state=42, n_init='auto')\n",
    "        kmeans.fit(df_scaled_features)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        # Silhouette score can be computationally expensive for large datasets\n",
    "        # If it's too slow, consider sampling: silhouette_score(df_scaled_features.sample(n=50000, random_state=42), kmeans.predict(df_scaled_features.sample(n=50000, random_state=42)))\n",
    "        score = silhouette_score(df_scaled_features, kmeans.labels_)\n",
    "        silhouette_scores.append(score)\n",
    "        print(f\"K={k_val}, Inertia: {kmeans.inertia_:.2f}, Silhouette Score: {score:.4f}\")\n",
    "\n",
    "    # Plotting Elbow Method and Silhouette Scores\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_range, inertia, marker='o')\n",
    "    plt.title('Elbow Method for Optimal K')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.xticks(k_range)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(k_range, silhouette_scores, marker='o')\n",
    "    plt.title('Silhouette Scores for Optimal K')\n",
    "    plt.xlabel('Number of Clusters (K)')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.xticks(k_range)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if silhouette_scores:\n",
    "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"\\nOptimal K based on highest Silhouette Score: {optimal_k}\")\n",
    "    else:\n",
    "        print(\"Could not determine optimal K from silhouette scores. Defaulting to K=3 or checking Elbow plot manually.\")\n",
    "        optimal_k = 3 # Fallback, user should check plots\n",
    "\n",
    "    # Apply K-Means with optimal K\n",
    "    print(f\"\\nApplying K-Means with K={optimal_k}...\")\n",
    "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init='auto')\n",
    "    cluster_labels = kmeans_final.fit_predict(df_scaled_features)\n",
    "    \n",
    "    # Add cluster labels to the (unscaled but filtered) aggregated features dataframe for easier interpretation\n",
    "    # df_processed_features should be the one before scaling but after filtering\n",
    "    df_clustered_devices = df_processed_features.copy() \n",
    "    df_clustered_devices['cluster'] = cluster_labels\n",
    "    \n",
    "    print(\"K-Means clustering applied. Cluster labels added.\")\n",
    "    print(df_clustered_devices.head().to_string())\n",
    "    print(\"\\nCluster sizes:\")\n",
    "    print(df_clustered_devices['cluster'].value_counts().sort_index())\n",
    "\n",
    "    # Save clustered data\n",
    "    # IMPORTANT: Update this path if you want to save to a different Drive location!\n",
    "    clustered_output_path = '/content/drive/MyDrive/path/to/your/coffee_device_clusters.csv'\n",
    "    print(f\"\\nSaving clustered device data to {clustered_output_path}...\")\n",
    "    try:\n",
    "        df_clustered_devices.reset_index().to_csv(clustered_output_path, sep=';', index=False) # save device_aid from index\n",
    "        print(f\"Clustered device data saved successfully to {clustered_output_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving clustered data CSV: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping K-Means application as scaled features are not available or empty.\")\n",
    "    df_clustered_devices = None # Ensure it's defined for the next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Interpretation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_clustered_devices' in locals() and df_clustered_devices is not None:\n",
    "    print(\"\\nCluster characteristics (mean values of features per cluster):\")\n",
    "    # Use df_processed_features (unscaled but filtered) for cluster_analysis if df_clustered_devices was based on it\n",
    "    # Or, if df_clustered_devices already has the correct (unscaled) features:\n",
    "    cluster_analysis = df_clustered_devices.groupby('cluster').mean()\n",
    "    print(cluster_analysis.to_string())\n",
    "\n",
    "    # Visualizations (adapted from restaurant_clustered.ipynb)\n",
    "    # Ensure 'kmeans_final' and 'df_scaled_features' are available from the previous cell for radar chart\n",
    "    if 'kmeans_final' in locals() and hasattr(kmeans_final, 'cluster_centers_') and \\\n",
    "       'df_scaled_features' in locals() and df_scaled_features is not None:\n",
    "        \n",
    "        scaled_cluster_centers = kmeans_final.cluster_centers_\n",
    "        feature_names_for_radar = df_scaled_features.columns.tolist()\n",
    "        num_clusters_viz = len(scaled_cluster_centers)\n",
    "\n",
    "        angles = np.linspace(0, 2 * np.pi, len(feature_names_for_radar), endpoint=False).tolist()\n",
    "        angles += angles[:1] # Close the plot\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "        for i in range(num_clusters_viz):\n",
    "            values = scaled_cluster_centers[i].tolist()\n",
    "            values += values[:1] # Close the plot\n",
    "            ax.plot(angles, values, linewidth=2, linestyle='solid', label=f\"Cluster {i}\")\n",
    "            ax.fill(angles, values, alpha=0.25)\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(feature_names_for_radar, size=8)\n",
    "        ax.set_title(\"Radar Chart of Scaled Cluster Centers\", size=16, y=1.1)\n",
    "        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "        # Save plot if needed: plt.savefig('radar_chart_coffee_clusters.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping radar chart: prerequisites (kmeans_final, df_scaled_features) not met.\")\n",
    "\n",
    "    # Box Plots for key features\n",
    "    # Select a few key features for box plots to avoid too many plots\n",
    "    key_features_for_boxplot = [\n",
    "        'avg_visit_duration_minutes',\n",
    "        'total_visits',\n",
    "        'num_unique_cafes_visited',\n",
    "        'avg_visits_per_week',\n",
    "        'total_time_spent_hours'\n",
    "    ] \n",
    "    # Add some rate features if they exist after filtering\n",
    "    for col in df_clustered_devices.columns:\n",
    "        if '_rate' in col and len(key_features_for_boxplot) < 8: # Limit total boxplots\n",
    "             if col in df_clustered_devices.columns: # Check if it survived filtering\n",
    "                key_features_for_boxplot.append(col)\n",
    "    \n",
    "    # Filter to existing columns only\n",
    "    key_features_for_boxplot = [f for f in key_features_for_boxplot if f in df_clustered_devices.columns]\n",
    "\n",
    "    if key_features_for_boxplot:\n",
    "        print(\"\\nGenerating box plots for key features...\")\n",
    "        num_features_to_plot = len(key_features_for_boxplot)\n",
    "        cols_subplot = 3 \n",
    "        rows_subplot = (num_features_to_plot + cols_subplot - 1) // cols_subplot\n",
    "        plt.figure(figsize=(15, rows_subplot * 5))\n",
    "        for i, feature in enumerate(key_features_for_boxplot):\n",
    "            plt.subplot(rows_subplot, cols_subplot, i + 1)\n",
    "            sns.boxplot(x='cluster', y=feature, data=df_clustered_devices, palette='viridis')\n",
    "            plt.title(f'{feature} by Cluster', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        # Save plot if needed: plt.savefig('box_plots_coffee_clusters.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping box plots: no key features selected or available.\")\n",
    "\n",
    "    # Bar chart of mean feature values (from cluster_analysis)\n",
    "    if not cluster_analysis.empty:\n",
    "        print(\"\\nGenerating bar chart of mean features...\")\n",
    "        cluster_analysis.T.plot(kind='bar', figsize=(18, 10), colormap='viridis') # Wider for more features\n",
    "        plt.title('Mean Feature Values by Cluster (Original Scale)')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Cluster')\n",
    "        plt.tight_layout()\n",
    "        # Save plot if needed: plt.savefig('bar_chart_mean_coffee_features.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Skipping bar chart: cluster_analysis is empty.\")\n",
    "else:\n",
    "    print(\"\\ndf_clustered_devices DataFrame not found. Skipping cluster analysis and visualization.\")\n",
    "\n",
    "print(\"\\n--- Coffee Clustering Notebook Execution Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
