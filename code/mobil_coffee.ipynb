{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Shop Visit Analysis\n",
    "\n",
    "This notebook performs an analysis to identify which coffee shops from `turkiye_geneli_kahve_zinciri_subeleri.csv` were visited by individuals based on mobility pings from `MobilityDataMay2024.parquet`.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Load mobility data (first 5 million rows) and coffee shop venue data.\n",
    "2. Convert both to GeoDataFrames.\n",
    "3. Create a 50-meter buffer around each coffee shop.\n",
    "4. Perform a spatial join to find mobility pings falling within these coffee shop buffers.\n",
    "5. Process visit data, adding time-based features (date, hour, day of week).\n",
    "6. Generate a summary table of visit frequencies.\n",
    "7. Save the detailed visit pings and the frequency summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "from shapely.geometry import Point\n",
    "import pyarrow # Required for parquet\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mount Google Drive (if running in Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted.\")\n",
    "    # Define base path for files on Google Drive\n",
    "    # IMPORTANT: Adjust this path if your files are in a subfolder of MyDrive\n",
    "    google_drive_base_path = '/content/drive/MyDrive/'\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, or google.colab module not found. Assuming files are local.\")\n",
    "    # Define a base path for local files (current directory)\n",
    "    google_drive_base_path = './' # Current directory if not in Colab\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    google_drive_base_path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Function for Coordinate Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coordinates(coord_series):\n",
    "    \"\"\"Cleans coordinate strings by removing single quotes, replacing commas with periods, and converting to numeric.\"\"\"\n",
    "    # Ensure series is string type, remove single quotes, then replace comma with period\n",
    "    cleaned_series = coord_series.astype(str).str.strip(\"'\").str.replace(',', '.', regex=False)\n",
    "    return pd.to_numeric(cleaned_series, errors='coerce')\n",
    "\n",
    "print(\"Helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading mobility data (MobilityDataMay2024.parquet)...\")\n",
    "# Construct path using the google_drive_base_path variable\n",
    "mobility_data_filename = 'MobilityDataMay2024.parquet'\n",
    "# Adjust path to be relative to the notebook's location in 'code/' directory, accessing parent directory for data files\n",
    "mobility_data_path = os.path.join(google_drive_base_path, '..', mobility_data_filename) if google_drive_base_path == './' else os.path.join(google_drive_base_path, mobility_data_filename)\n",
    "\n",
    "df_mobility = None\n",
    "try:\n",
    "    # Load the entire parquet file first\n",
    "    df_mobility_full = pd.read_parquet(mobility_data_path)\n",
    "    print(f\"Full mobility data loaded. Shape: {df_mobility_full.shape}\")\n",
    "    \n",
    "    # Take the first 5 million rows\n",
    "    num_rows_to_sample = 5000000\n",
    "    if len(df_mobility_full) > num_rows_to_sample:\n",
    "        df_mobility = df_mobility_full.head(num_rows_to_sample)\n",
    "        print(f\"Using the first {num_rows_to_sample} rows of mobility data. New shape: {df_mobility.shape}\")\n",
    "    else:\n",
    "        df_mobility = df_mobility_full\n",
    "        print(f\"Full mobility data has {len(df_mobility_full)} rows (less than or equal to {num_rows_to_sample}), using all of it. Shape: {df_mobility.shape}\")\n",
    "    del df_mobility_full # Free up memory\n",
    "\n",
    "    # Basic validation\n",
    "    if not all(col in df_mobility.columns for col in ['latitude', 'longitude', 'device_aid', 'timestamp']):\n",
    "        print(\"Error: Mobility data is missing one or more required columns: 'latitude', 'longitude', 'device_aid', 'timestamp'.\")\n",
    "        df_mobility = None # Invalidate df_mobility\n",
    "    else:\n",
    "        print(\"Required columns found in mobility data.\")\n",
    "        print(df_mobility.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mobility data file not found at {mobility_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading mobility data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Mobility GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_mobility = None\n",
    "if df_mobility is not None:\n",
    "    print(\"Creating mobility GeoDataFrame...\")\n",
    "    try:\n",
    "        gdf_mobility = geopandas.GeoDataFrame(\n",
    "            df_mobility,\n",
    "            geometry=geopandas.points_from_xy(df_mobility.longitude, df_mobility.latitude),\n",
    "            crs=\"EPSG:4326\"  # WGS84\n",
    "        )\n",
    "        print(f\"Mobility GeoDataFrame created. Shape: {gdf_mobility.shape}, CRS: {gdf_mobility.crs}\")\n",
    "        print(gdf_mobility.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating mobility GeoDataFrame: {e}\")\n",
    "else:\n",
    "    print(\"Skipping mobility GeoDataFrame creation as df_mobility was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Coffee Shop Venue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading coffee shop venue data (turkiye_geneli_kahve_zinciri_subeleri.csv)...\")\n",
    "# Construct path using the google_drive_base_path variable\n",
    "venue_data_filename = 'turkiye_geneli_kahve_zinciri_subeleri.csv'\n",
    "# Adjust path to be relative to the notebook's location in 'code/' directory, accessing parent directory for data files\n",
    "venue_data_path = os.path.join(google_drive_base_path, '..', venue_data_filename) if google_drive_base_path == './' else os.path.join(google_drive_base_path, venue_data_filename)\n",
    "\n",
    "df_venues = None\n",
    "try:\n",
    "    df_venues = pd.read_csv(venue_data_path, sep=';')\n",
    "    print(f\"Coffee shop venue data loaded. Shape: {df_venues.shape}\")\n",
    "    # Basic validation\n",
    "    if not all(col in df_venues.columns for col in ['latitude', 'longitude', 'isim']):\n",
    "        print(\"Error: Coffee shop venue data is missing one or more required columns: 'latitude', 'longitude', 'isim'.\")\n",
    "        df_venues = None # Invalidate df_venues\n",
    "    else:\n",
    "        print(\"Required columns found in coffee shop venue data.\")\n",
    "        print(df_venues.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Coffee shop venue data file not found at {venue_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading coffee shop venue data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean Venue Coordinates and Create Venue GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_venues = None\n",
    "if df_venues is not None:\n",
    "    print(\"Cleaning coffee shop venue coordinates and creating venue GeoDataFrame...\")\n",
    "    try:\n",
    "        df_venues['lat_cleaned'] = clean_coordinates(df_venues['latitude'])\n",
    "        df_venues['lng_cleaned'] = clean_coordinates(df_venues['longitude'])\n",
    "        \n",
    "        # Drop rows with invalid coordinates\n",
    "        original_venue_count = len(df_venues)\n",
    "        df_venues.dropna(subset=['lat_cleaned', 'lng_cleaned'], inplace=True)\n",
    "        print(f\"Dropped {original_venue_count - len(df_venues)} coffee shops due to invalid coordinates.\")\n",
    "\n",
    "        if df_venues.empty:\n",
    "            print(\"Error: No valid coffee shop venue coordinates after cleaning.\")\n",
    "        else:\n",
    "            gdf_venues = geopandas.GeoDataFrame(\n",
    "                df_venues,\n",
    "                geometry=geopandas.points_from_xy(df_venues.lng_cleaned, df_venues.lat_cleaned),\n",
    "                crs=\"EPSG:4326\"  # WGS84\n",
    "            )\n",
    "            print(f\"Coffee shop venue GeoDataFrame created. Shape: {gdf_venues.shape}, CRS: {gdf_venues.crs}\")\n",
    "            print(gdf_venues.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating coffee shop venue GeoDataFrame: {e}\")\n",
    "else:\n",
    "    print(\"Skipping coffee shop venue GeoDataFrame creation as df_venues was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spatial Analysis: Buffering and Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_visits = None\n",
    "if gdf_venues is not None and gdf_mobility is not None:\n",
    "    # Target CRS for buffering and spatial join (UTM Zone 36N, suitable for Istanbul/Western Turkey)\n",
    "    # If data covers a wider area, a more dynamic UTM zone selection or a different projected CRS might be needed.\n",
    "    projected_crs = \"EPSG:32636\" \n",
    "    buffer_radius_meters = 50\n",
    "\n",
    "    print(f\"Projecting coffee shop venue data to {projected_crs} for buffering...\")\n",
    "    try:\n",
    "        gdf_venues_projected = gdf_venues.to_crs(projected_crs)\n",
    "        print(f\"Coffee shop venue data projected. CRS: {gdf_venues_projected.crs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error projecting coffee shop venue data: {e}\")\n",
    "        gdf_venues_projected = None\n",
    "\n",
    "    gdf_venue_buffers = None\n",
    "    if gdf_venues_projected is not None:\n",
    "        print(f\"Creating {buffer_radius_meters}m buffers around coffee shops...\")\n",
    "        try:\n",
    "            # Ensure the geometry column is active and valid before buffering\n",
    "            if not gdf_venues_projected.geometry.is_valid.all():\n",
    "                print(\"Warning: Some coffee shop venue geometries are invalid. Attempting to fix...\")\n",
    "                # A common trick to fix invalid geometries; may not always work perfectly.\n",
    "                gdf_venues_projected.geometry = gdf_venues_projected.geometry.buffer(0) \n",
    "                if not gdf_venues_projected.geometry.is_valid.all():\n",
    "                     print(\"Error: Could not fix all invalid coffee shop venue geometries. Proceeding with potentially problematic data.\")\n",
    "            \n",
    "            gdf_venue_buffers = gdf_venues_projected.copy()\n",
    "            gdf_venue_buffers['geometry'] = gdf_venues_projected.geometry.buffer(buffer_radius_meters)\n",
    "            print(f\"Coffee shop venue buffers created. Shape: {gdf_venue_buffers.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating coffee shop venue buffers: {e}\")\n",
    "            gdf_venue_buffers = None\n",
    "\n",
    "    gdf_mobility_projected = None\n",
    "    if gdf_venue_buffers is not None: # Proceed only if buffers were created\n",
    "        print(f\"Projecting mobility data to {projected_crs} for spatial join...\")\n",
    "        try:\n",
    "            gdf_mobility_projected = gdf_mobility.to_crs(projected_crs)\n",
    "            print(f\"Mobility data projected. CRS: {gdf_mobility_projected.crs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error projecting mobility data: {e}\")\n",
    "            gdf_mobility_projected = None\n",
    "\n",
    "    if gdf_mobility_projected is not None and gdf_venue_buffers is not None:\n",
    "        print(\"Performing spatial join (mobile pings within coffee shop venue buffers)...\")\n",
    "        try:\n",
    "            # Use all columns from gdf_venue_buffers for the join to retain all venue information\n",
    "            gdf_venue_buffers_for_join = gdf_venue_buffers.copy()\n",
    "            print(f\"Columns in gdf_venue_buffers_for_join before sjoin: {gdf_venue_buffers_for_join.columns.tolist()}\")\n",
    "            \n",
    "            # Perform the spatial join\n",
    "            # 'predicate=\"within\"' means mobility points must be within venue buffers\n",
    "            gdf_visits = geopandas.sjoin(gdf_mobility_projected, gdf_venue_buffers_for_join, how='inner', predicate='within')\n",
    "            print(f\"Spatial join completed. Number of potential visit pings: {gdf_visits.shape[0]}\")\n",
    "            if gdf_visits.empty:\n",
    "                print(\"No visits found after spatial join.\")\n",
    "            else:\n",
    "                print(\"Sample of joined visit data (first 5 rows):\")\n",
    "                print(gdf_visits.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during spatial join: {e}\")\n",
    "            gdf_visits = None\n",
    "else:\n",
    "    print(\"Skipping spatial analysis as one or both GeoDataFrames (mobility, venues) are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Results, Add Time Features, and Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdf_visits is not None and not gdf_visits.empty:\n",
    "    print(\"Extracting relevant columns and adding time features for the final output...\")\n",
    "    \n",
    "    df_visits_output = gdf_visits.copy()\n",
    "    \n",
    "    try:\n",
    "        df_visits_output['original_latitude'] = df_mobility.loc[df_visits_output.index, 'latitude'].values\n",
    "        df_visits_output['original_longitude'] = df_mobility.loc[df_visits_output.index, 'longitude'].values\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError while trying to map original lat/lon: {e}. Original coordinates might be missing.\")\n",
    "        df_visits_output['original_latitude'] = pd.NA\n",
    "        df_visits_output['original_longitude'] = pd.NA\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while mapping original lat/lon: {e}\")\n",
    "        df_visits_output['original_latitude'] = pd.NA\n",
    "        df_visits_output['original_longitude'] = pd.NA\n",
    "    \n",
    "    # Convert timestamp to datetime and extract time features\n",
    "    df_visits_output['timestamp'] = pd.to_datetime(df_visits_output['timestamp'])\n",
    "    df_visits_output['date'] = df_visits_output['timestamp'].dt.date\n",
    "    df_visits_output['hour_of_day'] = df_visits_output['timestamp'].dt.hour\n",
    "    df_visits_output['day_of_week'] = df_visits_output['timestamp'].dt.day_name()\n",
    "\n",
    "    # Columns from mobility data (device_aid, timestamp, original lat/lon of the ping, and new time features)\n",
    "    mobility_cols_to_keep = ['device_aid', 'timestamp', 'original_latitude', 'original_longitude', 'date', 'hour_of_day', 'day_of_week']\n",
    "\n",
    "    # Original columns from df_venues (loaded from turkiye_geneli_kahve_zinciri_subeleri.csv)\n",
    "    # We want 'isim', 'adres', 'google_maps_link'\n",
    "    original_venue_column_names = ['isim', 'adres', 'google_maps_link']\n",
    "    \n",
    "    processed_venue_columns_for_output = []\n",
    "    for venue_col_original_name in original_venue_column_names:\n",
    "        if venue_col_original_name in df_visits_output.columns:\n",
    "            processed_venue_columns_for_output.append(venue_col_original_name)\n",
    "        elif f\"{venue_col_original_name}_right\" in df_visits_output.columns:\n",
    "            df_visits_output.rename(columns={f\"{venue_col_original_name}_right\": venue_col_original_name}, inplace=True)\n",
    "            processed_venue_columns_for_output.append(venue_col_original_name)\n",
    "\n",
    "    final_output_columns_list = mobility_cols_to_keep + processed_venue_columns_for_output\n",
    "    \n",
    "    columns_to_exclude_finally = ['geometry_right', 'index_right', 'lat_cleaned', 'lng_cleaned', 'latitude', 'longitude'] # Exclude raw and cleaned venue coords\n",
    "    \n",
    "    final_selected_columns = []\n",
    "    seen_columns = set()\n",
    "    for col in final_output_columns_list:\n",
    "        if col in df_visits_output.columns and col not in columns_to_exclude_finally and col not in seen_columns:\n",
    "            final_selected_columns.append(col)\n",
    "            seen_columns.add(col)\n",
    "            \n",
    "    print(f\"Final columns selected for detailed output: {final_selected_columns}\")\n",
    "        \n",
    "    df_final_visits_log = df_visits_output[final_selected_columns]\n",
    "\n",
    "    output_log_filename = 'mobil_coffee.csv'\n",
    "    # Adjust path for saving output file in the main directory, not 'code/'\n",
    "    output_log_path = os.path.join(google_drive_base_path, '..', output_log_filename) if google_drive_base_path == './' else os.path.join(google_drive_base_path, output_log_filename)\n",
    "    print(f\"Saving detailed visit log to {output_log_path}...\")\n",
    "    try:\n",
    "        df_final_visits_log.to_csv(output_log_path, index=False, sep=';')\n",
    "        print(f\"Successfully saved detailed visit log to {output_log_path}. Shape: {df_final_visits_log.shape}\")\n",
    "        print(df_final_visits_log.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving detailed visit log: {e}\")\n",
    "    \n",
    "    # Generate and save visit frequency summary\n",
    "    print(\"Generating visit frequency summary...\")\n",
    "    try:\n",
    "        # Ensure 'isim' is present for grouping (it should be if processed_venue_columns_for_output included it)\n",
    "        if 'isim' in df_visits_output.columns:\n",
    "            df_visit_summary = df_visits_output.groupby(['device_aid', 'isim', 'date', 'hour_of_day']).size().reset_index(name='visit_ping_count')\n",
    "            output_summary_filename = 'mobil_coffee_visit_summary.csv'\n",
    "            # Adjust path for saving output file in the main directory\n",
    "            output_summary_path = os.path.join(google_drive_base_path, '..', output_summary_filename) if google_drive_base_path == './' else os.path.join(google_drive_base_path, output_summary_filename)\n",
    "            print(f\"Saving visit frequency summary to {output_summary_path}...\")\n",
    "            df_visit_summary.to_csv(output_summary_path, index=False, sep=';')\n",
    "            print(f\"Successfully saved visit frequency summary to {output_summary_path}. Shape: {df_visit_summary.shape}\")\n",
    "            print(df_visit_summary.head())\n",
    "        else:\n",
    "            print(\"Error: 'isim' column not found in df_visits_output. Cannot generate frequency summary.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating or saving visit frequency summary: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No visit data to save (either gdf_visits is None or empty).\")\n",
    "\n",
    "print(\"Coffee shop visit analysis script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
