{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilite Verisi Özellik Çıkarımı\n",
    "\n",
    "Bu not defteri, `final_mobility_analysis_results.csv` dosyasındaki işlenmiş mobilite verilerinden cihaz (device_aid) bazında özellikler çıkarır.\n",
    "Amaç, her bir cihazın davranışlarını ve tercihlerini yansıtan bir özellik seti oluşturmaktır.\n",
    "Bu özellikler daha sonra persona oluşturma gibi analizlerde kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ortam Kurulumu ve Kütüphanelerin İçe Aktarılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Bağlantısı ve Dosya Yolları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "# Lütfen GEREKİRSE aşağıdaki dosya yollarını kendi Google Drive yapınıza göre güncelleyin:\n",
    "BASE_DRIVE_PATH = '/content/drive/MyDrive/' # Genellikle '/content/drive/MyDrive/' olur\n",
    "\n",
    "INPUT_CSV_NAME = 'final_mobility_analysis_results.csv'\n",
    "OUTPUT_FEATURES_CSV_NAME = 'device_features_v2.csv' # Updated output name\n",
    "\n",
    "input_file_path = os.path.join(BASE_DRIVE_PATH, INPUT_CSV_NAME)\n",
    "output_file_path = os.path.join(BASE_DRIVE_PATH, OUTPUT_FEATURES_CSV_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Veri Yükleme ve Ön Hazırlık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_results = pd.read_csv(input_file_path)\n",
    "    print(f\"'{INPUT_CSV_NAME}' başarıyla yüklendi. Boyut: {df_results.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"HATA: Giriş dosyası bulunamadı: {input_file_path}\")\n",
    "    df_results = None\n",
    "except Exception as e:\n",
    "    print(f\"HATA: Veri yüklenirken bir sorun oluştu: {e}\")\n",
    "    df_results = None\n",
    "\n",
    "if df_results is not None:\n",
    "    if 'timestamp_dt' in df_results.columns and not pd.api.types.is_datetime64_any_dtype(df_results['timestamp_dt']):\n",
    "        df_results['timestamp_dt'] = pd.to_datetime(df_results['timestamp_dt'])\n",
    "    elif 'readable_time' in df_results.columns and not pd.api.types.is_datetime64_any_dtype(df_results['readable_time']):\n",
    "        df_results['timestamp_dt'] = pd.to_datetime(df_results['readable_time'])\n",
    "    elif 'timestamp' in df_results.columns:\n",
    "        df_results['timestamp_dt'] = pd.to_datetime(df_results['timestamp'], unit='s')\n",
    "    else:\n",
    "        print(\"HATA: Geçerli bir zaman damgası sütunu bulunamadı.\")\n",
    "        df_results = None\n",
    "    \n",
    "    if df_results is not None and 'timestamp_dt' in df_results.columns:\n",
    "        print(\"\\nZaman damgası sütunu ('timestamp_dt') başarıyla datetime formatına getirildi.\")\n",
    "        original_df_results_cols = df_results.columns.tolist() # Store original columns before cleaning\n",
    "        df_results.columns = df_results.columns.str.replace('[^A-Za-z0-9_]+', '', regex=True)\n",
    "        print(\"Sütun adları temizlendi.\")\n",
    "        \n",
    "        polygon_info_map = {\n",
    "            'airport': 'airport_NAME', # Re-included\n",
    "            'poi': 'poi_NAME',\n",
    "            # 'lvl1': 'lvl1_NAME', # REMOVED\n",
    "            # 'lvl2': 'lvl2_NAME', # REMOVED\n",
    "            # 'lvl3': 'lvl3_NAME', # REMOVED\n",
    "            # 'lvl4': 'lvl4_NAME', # REMOVED\n",
    "            'clubs': 'clubs_Network',  # Re-included\n",
    "            'hotels': 'hotels_MusteriTabelaAdi',\n",
    "            'luxury_houses': 'luxuryhouses_POIAd', # Assuming 'luxuryhouses_POIAd' is correct from previous version\n",
    "            'turkey_sites': 'turkeysites_KonutSiteleri',\n",
    "            'p_schools': 'pschools_name'\n",
    "        }\n",
    "        valid_polygon_info = {}\n",
    "        for p_short, name_col_original in polygon_info_map.items():\n",
    "            in_col = f'in_{p_short}'\n",
    "            name_col_cleaned = name_col_original \n",
    "            if in_col in df_results.columns:\n",
    "                if name_col_cleaned and name_col_cleaned in df_results.columns:\n",
    "                    valid_polygon_info[p_short] = {'in_col': in_col, 'name_col': name_col_cleaned}\n",
    "                else:\n",
    "                    valid_polygon_info[p_short] = {'in_col': in_col, 'name_col': None}\n",
    "                    if name_col_cleaned:\n",
    "                         print(f\"Uyarı: '{p_short}' için isim sütunu '{name_col_cleaned}' bulunamadı, ancak '{in_col}' var.\")\n",
    "        \n",
    "        polygon_cols_present = [info['in_col'] for p_short, info in valid_polygon_info.items()]\n",
    "        print(f\"\\nAnalizde kullanılacak 'in_' poligon sütunları: {polygon_cols_present}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Özellik Çıkarımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_results is not None and 'device_aid' in df_results.columns and 'timestamp_dt' in df_results.columns:\n",
    "    df_features = pd.DataFrame(index=df_results['device_aid'].unique())\n",
    "    print(f\"Özellik DataFrame'i {len(df_features.index)} benzersiz device_aid için oluşturuldu.\")\n",
    "\n",
    "    # --- I. Temel Aktivite Özellikleri ---\n",
    "    print(\"\\nI. Temel Aktivite Özellikleri çıkarılıyor...\")\n",
    "    df_features['total_pings'] = df_results.groupby('device_aid').size()\n",
    "    df_features['unique_days_active'] = df_results.groupby('device_aid')['timestamp_dt'].apply(lambda x: x.dt.date.nunique())\n",
    "    df_features['activity_span_days'] = df_results.groupby('device_aid')['timestamp_dt'].apply(lambda x: (x.max() - x.min()).days if pd.notnull(x.max()) and pd.notnull(x.min()) else 0)\n",
    "\n",
    "    # --- II. Lokasyon Bazlı Yoğunluk Özellikleri ---\n",
    "    print(\"\\nII. Lokasyon Bazlı Yoğunluk Özellikleri çıkarılıyor...\")\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        if df_results[in_col].dtype == 'object': \n",
    "            df_results[in_col] = df_results[in_col].map({'True': 1, 'False': 0, True: 1, False: 0}).fillna(0)\n",
    "        elif df_results[in_col].dtype == 'bool':\n",
    "            df_results[in_col] = df_results[in_col].astype(int)\n",
    "        \n",
    "        df_features[f'pings_in_{p_short}'] = df_results.groupby('device_aid')[in_col].sum().astype(int)\n",
    "        df_features[f'ratio_in_{p_short}'] = (df_features[f'pings_in_{p_short}'] / df_features['total_pings']).fillna(0)\n",
    "\n",
    "    # --- III. Lokasyon Çeşitliliği Özellikleri ---\n",
    "    print(\"\\nIII. Lokasyon Çeşitliliği Özellikleri çıkarılıyor...\")\n",
    "    ping_cols_for_diversity = [f'pings_in_{p_short}' for p_short in valid_polygon_info.keys() if f'pings_in_{p_short}' in df_features.columns]\n",
    "    df_features['num_distinct_polygon_types_visited'] = df_features[ping_cols_for_diversity].gt(0).sum(axis=1)\n",
    "\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        name_col = info['name_col']\n",
    "        feature_col_name = f'num_distinct_{p_short}'\n",
    "        if name_col and name_col in df_results.columns: \n",
    "            df_in_polygon = df_results[df_results[in_col] == 1]\n",
    "            if not df_in_polygon.empty:\n",
    "                 distinct_counts = df_in_polygon.groupby('device_aid')[name_col].nunique()\n",
    "                 df_features[feature_col_name] = distinct_counts\n",
    "                 df_features[feature_col_name].fillna(0, inplace=True)\n",
    "            else:\n",
    "                 df_features[feature_col_name] = 0\n",
    "        else:\n",
    "            df_features[feature_col_name] = 0 \n",
    "\n",
    "    # --- IV. Zaman Dilimi Bazlı Genel Aktivite Özellikleri ---\n",
    "    print(\"\\nIV. Zaman Dilimi Bazlı Genel Aktivite Özellikleri çıkarılıyor...\")\n",
    "    df_results['hour'] = df_results['timestamp_dt'].dt.hour\n",
    "    time_bins = [0, 6, 12, 18, 24]\n",
    "    time_labels = ['gece', 'sabah', 'ogle', 'aksam'] \n",
    "    df_results['time_of_day'] = pd.cut(df_results['hour'], bins=time_bins, labels=time_labels, right=False, include_lowest=True)\n",
    "\n",
    "    tod_counts = df_results.groupby(['device_aid', 'time_of_day'], observed=False).size().unstack(fill_value=0)\n",
    "    for label in time_labels:\n",
    "        if label in tod_counts.columns:\n",
    "            df_features[f'pings_{label}'] = tod_counts[label]\n",
    "            df_features[f'ratio_{label}_pings'] = (df_features[f'pings_{label}'] / df_features['total_pings']).fillna(0)\n",
    "        else:\n",
    "            df_features[f'pings_{label}'] = 0\n",
    "            df_features[f'ratio_{label}_pings'] = 0\n",
    "\n",
    "    # --- V. Zaman Dilimi ve Lokasyon Bazlı Kombine Özellikler ---\n",
    "    print(\"\\nV. Zaman Dilimi ve Lokasyon Bazlı Kombine Özellikler çıkarılıyor...\")\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        p_tod_counts = df_results[df_results[in_col] == 1].groupby(['device_aid', 'time_of_day'], observed=False).size().unstack(fill_value=0)\n",
    "        for t_label in time_labels:\n",
    "            col_name_p_t = f'pings_{p_short}_{t_label}'\n",
    "            if t_label in p_tod_counts.columns:\n",
    "                df_features[col_name_p_t] = p_tod_counts[t_label]\n",
    "            else:\n",
    "                df_features[col_name_p_t] = 0\n",
    "            df_features[f'ratio_{p_short}_{t_label}_to_total_in_{p_short}'] = \\\n",
    "                (df_features[col_name_p_t] / df_features[f'pings_in_{p_short}'].replace(0, np.nan)).fillna(0)\n",
    "            df_features[f'ratio_{p_short}_{t_label}_to_total_device_pings'] = \\\n",
    "                (df_features[col_name_p_t] / df_features['total_pings']).fillna(0)\n",
    "\n",
    "    # --- VI. Potansiyel İkametgah Göstergesi (Gece Sinyalleri) ---\n",
    "    print(\"\\nVI. Potansiyel İkametgah Göstergesi (Gece Sinyalleri) çıkarılıyor...\")\n",
    "    df_night_pings_by_type = pd.DataFrame(index=df_features.index)\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        night_pings_in_p_type = df_results[(df_results['time_of_day'] == 'gece') & (df_results[in_col] == 1)]\\\n",
    "                                  .groupby('device_aid').size()\n",
    "        df_night_pings_by_type[p_short] = night_pings_in_p_type\n",
    "    df_night_pings_by_type.fillna(0, inplace=True)\n",
    "\n",
    "    if not df_night_pings_by_type.empty:\n",
    "        df_features['dominant_gece_location_type'] = df_night_pings_by_type.idxmax(axis=1)\n",
    "        df_features['dominant_gece_location_ping_count'] = df_night_pings_by_type.max(axis=1).astype(int)\n",
    "        df_features.loc[df_night_pings_by_type.sum(axis=1) == 0, 'dominant_gece_location_type'] = np.nan\n",
    "        df_features.loc[df_night_pings_by_type.sum(axis=1) == 0, 'dominant_gece_location_ping_count'] = 0\n",
    "        df_features['ratio_dominant_gece_loc_pings_to_total_gece'] = \\\n",
    "            (df_features['dominant_gece_location_ping_count'] / df_features['pings_gece'].replace(0, np.nan)).fillna(0)\n",
    "    else:\n",
    "        df_features['dominant_gece_location_type'] = np.nan\n",
    "        df_features['dominant_gece_location_ping_count'] = 0\n",
    "        df_features['ratio_dominant_gece_loc_pings_to_total_gece'] = 0\n",
    "\n",
    "    df_night_all_devices = df_results[df_results['time_of_day'] == 'gece'].copy()\n",
    "    def get_dominant_night_loc_name_apply(device_series):\n",
    "        device_id = device_series.name\n",
    "        dominant_type_short = device_series['dominant_gece_location_type']\n",
    "        if pd.isna(dominant_type_short) or dominant_type_short not in valid_polygon_info or valid_polygon_info[dominant_type_short]['name_col'] is None:\n",
    "            return np.nan\n",
    "        in_col = valid_polygon_info[dominant_type_short]['in_col']\n",
    "        name_col = valid_polygon_info[dominant_type_short]['name_col']\n",
    "        device_night_pings_in_dom_type = df_night_all_devices[\n",
    "            (df_night_all_devices['device_aid'] == device_id) & (df_night_all_devices[in_col] == 1)\n",
    "        ]\n",
    "        if device_night_pings_in_dom_type.empty or not device_night_pings_in_dom_type[name_col].notna().any():\n",
    "            return np.nan\n",
    "        mode_series = device_night_pings_in_dom_type[name_col].mode()\n",
    "        return mode_series.iloc[0] if not mode_series.empty else np.nan\n",
    "    if not df_features.empty and 'dominant_gece_location_type' in df_features.columns:\n",
    "        df_features['dominant_gece_location_name'] = df_features.apply(get_dominant_night_loc_name_apply, axis=1)\n",
    "    else:\n",
    "        df_features['dominant_gece_location_name'] = np.nan\n",
    "    df_features['is_gece_location_consistent_type'] = df_features.get('ratio_dominant_gece_loc_pings_to_total_gece', pd.Series(0, index=df_features.index))\n",
    "    def get_pings_at_dominant_name(device_series):\n",
    "        device_id = device_series.name\n",
    "        dom_name = device_series['dominant_gece_location_name']\n",
    "        dom_type_short = device_series['dominant_gece_location_type']\n",
    "        if pd.isna(dom_name) or pd.isna(dom_type_short) or dom_type_short not in valid_polygon_info or valid_polygon_info[dom_type_short]['name_col'] is None:\n",
    "            return 0\n",
    "        in_col = valid_polygon_info[dom_type_short]['in_col']\n",
    "        name_col = valid_polygon_info[dom_type_short]['name_col']\n",
    "        pings = df_night_all_devices[\n",
    "            (df_night_all_devices['device_aid'] == device_id) &\n",
    "            (df_night_all_devices[in_col] == 1) &\n",
    "            (df_night_all_devices[name_col] == dom_name)\n",
    "        ].shape[0]\n",
    "        return pings\n",
    "    if not df_features.empty and 'dominant_gece_location_name' in df_features.columns:\n",
    "        pings_at_dom_name = df_features.apply(get_pings_at_dominant_name, axis=1)\n",
    "        df_features['is_gece_location_consistent_name'] = (pings_at_dom_name / df_features['pings_gece'].replace(0, np.nan)).fillna(0)\n",
    "    else:\n",
    "        df_features['is_gece_location_consistent_name'] = 0\n",
    "\n",
    "    # --- VII. Hafta İçi/Hafta Sonu Aktivite Özellikleri ---\n",
    "    print(\"\\nVII. Hafta İçi/Hafta Sonu Aktivite Özellikleri çıkarılıyor...\")\n",
    "    df_results['day_of_week'] = df_results['timestamp_dt'].dt.dayofweek \n",
    "    df_results['is_weekend'] = df_results['day_of_week'] >= 5 \n",
    "    weekend_counts = df_results.groupby(['device_aid', 'is_weekend'], observed=False).size().unstack(fill_value=0)\n",
    "    if False in weekend_counts.columns: df_features['pings_weekday'] = weekend_counts[False]\n",
    "    else: df_features['pings_weekday'] = 0\n",
    "    if True in weekend_counts.columns: df_features['pings_weekend'] = weekend_counts[True]\n",
    "    else: df_features['pings_weekend'] = 0\n",
    "    df_features['ratio_weekend_pings'] = (df_features['pings_weekend'] / df_features['total_pings']).fillna(0)\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        p_weekend_counts = df_results[df_results[in_col] == 1].groupby(['device_aid', 'is_weekend'], observed=False).size().unstack(fill_value=0)\n",
    "        col_name_p_wd = f'pings_{p_short}_weekday'\n",
    "        col_name_p_we = f'pings_{p_short}_weekend'\n",
    "        if False in p_weekend_counts.columns: df_features[col_name_p_wd] = p_weekend_counts[False]\n",
    "        else: df_features[col_name_p_wd] = 0\n",
    "        if True in p_weekend_counts.columns: df_features[col_name_p_we] = p_weekend_counts[True]\n",
    "        else: df_features[col_name_p_we] = 0\n",
    "        df_features[f'ratio_{p_short}_weekend_to_total_{p_short}'] = \\\n",
    "            (df_features[col_name_p_we] / df_features[f'pings_in_{p_short}'].replace(0, np.nan)).fillna(0)\n",
    "    \n",
    "    # --- VIII. Top Ziyaret Edilen Spesifik Mekanlar ---\n",
    "    print(\"\\nVIII. Top Ziyaret Edilen Spesifik Mekanlar özellikleri çıkarılıyor...\")\n",
    "    all_locations_data = []\n",
    "    for p_short, info in valid_polygon_info.items(): # This loop processes all valid polygons\n",
    "        in_col = info['in_col']\n",
    "        name_col = info['name_col']\n",
    "        if name_col and name_col in df_results.columns: \n",
    "            df_subset = df_results[(df_results[in_col] == 1) & df_results[name_col].notna()].copy()\n",
    "            if not df_subset.empty:\n",
    "                df_subset['location_type_derived'] = p_short\n",
    "                temp_df = df_subset[['device_aid', name_col, 'location_type_derived']].rename(columns={name_col: 'location_name'})\n",
    "                all_locations_data.append(temp_df)\n",
    "    \n",
    "    cleaned_neighborhood_col = 'neighborhood' \n",
    "    if 'neighborhood' not in df_results.columns:\n",
    "        for orig_col_idx, orig_col_name_in_file in enumerate(original_df_results_cols):\n",
    "            if orig_col_name_in_file == 'neighborhood': \n",
    "                if orig_col_idx < len(df_results.columns):\n",
    "                    cleaned_neighborhood_col = df_results.columns[orig_col_idx]\n",
    "                    if 'neighborhood' not in cleaned_neighborhood_col: \n",
    "                        print(f\"Uyarı: Orijinal 'neighborhood' sütunu, '{cleaned_neighborhood_col}' olarak temizlendi.\")\n",
    "                break\n",
    "        else: \n",
    "            cleaned_neighborhood_col = None \n",
    "            print(\"Uyarı: 'neighborhood' sütunu bulunamadı.\")\n",
    "    \n",
    "    if cleaned_neighborhood_col and cleaned_neighborhood_col in df_results.columns and df_results[cleaned_neighborhood_col].notna().any():\n",
    "        df_subset_neighborhood = df_results[df_results[cleaned_neighborhood_col].notna()].copy()\n",
    "        df_subset_neighborhood['location_type_derived'] = 'neighborhood'\n",
    "        temp_df_neighborhood = df_subset_neighborhood[['device_aid', cleaned_neighborhood_col, 'location_type_derived']].rename(columns={cleaned_neighborhood_col: 'location_name'})\n",
    "        all_locations_data.append(temp_df_neighborhood)\n",
    "    else:\n",
    "        print(f\"Uyarı: Mahalle sütunu ('{cleaned_neighborhood_col}') bulunamadı veya tamamı boş. Top mekanlara dahil edilmeyecek.\")\n",
    "\n",
    "    N_TOP_LOCATIONS = 3\n",
    "    top_n_cols_base = []\n",
    "    for i in range(1, N_TOP_LOCATIONS + 1):\n",
    "        top_n_cols_base.extend([f'top_{i}_loc_name', f'top_{i}_loc_type', f'top_{i}_loc_pings'])\n",
    "    for col in top_n_cols_base: df_features[col] = np.nan \n",
    "\n",
    "    if all_locations_data:\n",
    "        df_all_named_locations = pd.concat(all_locations_data, ignore_index=True)\n",
    "        if not df_all_named_locations.empty:\n",
    "            location_ping_counts = df_all_named_locations.groupby(['device_aid', 'location_name', 'location_type_derived'], observed=False).size().reset_index(name='ping_count')\n",
    "            \n",
    "            def get_top_n_locations_flat(group, cols_to_fill):\n",
    "                top_n = group.sort_values('ping_count', ascending=False).head(N_TOP_LOCATIONS)\n",
    "                records = []\n",
    "                for i_row in range(N_TOP_LOCATIONS):\n",
    "                    if i_row < len(top_n):\n",
    "                        row = top_n.iloc[i_row]\n",
    "                        records.extend([row.location_name, row.location_type_derived, row.ping_count])\n",
    "                    else:\n",
    "                        records.extend([np.nan, np.nan, np.nan]) \n",
    "                return pd.Series(records, index=cols_to_fill)\n",
    "\n",
    "            if not location_ping_counts.empty:\n",
    "                top_n_features_df = location_ping_counts.groupby('device_aid', group_keys=False).apply(get_top_n_locations_flat, cols_to_fill=top_n_cols_base)\n",
    "                if not top_n_features_df.empty:\n",
    "                    # Directly assign/update columns in df_features, aligning by index\n",
    "                    for col_to_assign in top_n_features_df.columns:\n",
    "                        df_features[col_to_assign] = top_n_features_df[col_to_assign].reindex(df_features.index)\n",
    "                    \n",
    "                    # Ensure correct types for ping counts and handle NaNs\n",
    "                    for i in range(1, N_TOP_LOCATIONS + 1):\n",
    "                        ping_col = f'top_{i}_loc_pings'\n",
    "                        if ping_col in df_features.columns:\n",
    "                           df_features[ping_col] = df_features[ping_col].fillna(0).astype(int)\n",
    "                else:\n",
    "                     print(\"Uyarı: top_n_features_df DataFrame'i boş, top N mekan özellikleri eklenemedi.\")\n",
    "            else:\n",
    "                print(\"Uyarı: location_ping_counts DataFrame'i boş, top N mekan özellikleri eklenemedi.\")\n",
    "        else:\n",
    "            print(\"Uyarı: df_all_named_locations DataFrame'i boş, top N mekan özellikleri eklenemedi.\")\n",
    "    else:\n",
    "        print(\"Uyarı: En çok ziyaret edilen mekan özellikleri için isimlendirilmiş lokasyon verisi bulunamadı.\")\n",
    "    \n",
    "    # --- IX. Top Neighborhood Time-of-Day Analysis ---\n",
    "    print(\"\\nIX. Top Neighborhood Time-of-Day Analysis özellikleri çıkarılıyor...\")\n",
    "    top_neighborhood_name_col_feat = 'top_neighborhood_name' # Name for feature df\n",
    "    top_neighborhood_pings_col_feat = 'top_neighborhood_pings_total'\n",
    "    \n",
    "    # Initialize columns in df_features\n",
    "    df_features[top_neighborhood_name_col_feat] = np.nan\n",
    "    df_features[top_neighborhood_pings_col_feat] = 0\n",
    "    for t_label in time_labels:\n",
    "        df_features[f'top_neighborhood_pings_{t_label}'] = 0\n",
    "        df_features[f'ratio_top_neighborhood_{t_label}_to_total_top_neighborhood'] = 0\n",
    "\n",
    "    if cleaned_neighborhood_col and cleaned_neighborhood_col in df_results.columns and df_results[cleaned_neighborhood_col].notna().any():\n",
    "        neighborhood_ping_counts_for_top = df_results.groupby(['device_aid', cleaned_neighborhood_col], observed=False).size().reset_index(name='pings_in_neighborhood')\n",
    "        if not neighborhood_ping_counts_for_top.empty:\n",
    "            # Find the neighborhood with max pings for each device_aid\n",
    "            idx_top_hood = neighborhood_ping_counts_for_top.groupby(['device_aid'])['pings_in_neighborhood'].idxmax()\n",
    "            top_neighborhood_info_df = neighborhood_ping_counts_for_top.loc[idx_top_hood].set_index('device_aid')\n",
    "            \n",
    "            # Assign to df_features, ensuring index alignment\n",
    "            df_features[top_neighborhood_name_col_feat] = top_neighborhood_info_df[cleaned_neighborhood_col].reindex(df_features.index)\n",
    "            df_features[top_neighborhood_pings_col_feat] = top_neighborhood_info_df['pings_in_neighborhood'].reindex(df_features.index).fillna(0).astype(int)\n",
    "\n",
    "            # Merge top neighborhood name back to df_results to filter pings for time-of-day analysis\n",
    "            # Use a temporary df_results with only necessary columns for merge efficiency\n",
    "            df_results_for_merge = df_results[['device_aid', cleaned_neighborhood_col, 'time_of_day']].copy()\n",
    "            df_results_temp = df_results_for_merge.merge(df_features[[top_neighborhood_name_col_feat]].reset_index(), on='device_aid', how='left')\n",
    "            \n",
    "            # Filter pings that occurred in each device's top neighborhood\n",
    "            df_pings_in_top_hood = df_results_temp[df_results_temp[cleaned_neighborhood_col] == df_results_temp[top_neighborhood_name_col_feat]]\n",
    "\n",
    "            if not df_pings_in_top_hood.empty:\n",
    "                top_hood_tod_counts = df_pings_in_top_hood.groupby(['device_aid', 'time_of_day'], observed=False).size().unstack(fill_value=0)\n",
    "                for t_label in time_labels:\n",
    "                    col_name = f'top_neighborhood_pings_{t_label}'\n",
    "                    ratio_col_name = f'ratio_top_neighborhood_{t_label}_to_total_top_neighborhood'\n",
    "                    if t_label in top_hood_tod_counts.columns:\n",
    "                        # Map results to df_features, aligning by index (device_aid)\n",
    "                        df_features[col_name] = df_features.index.map(top_hood_tod_counts[t_label]).fillna(0).astype(int)\n",
    "                        df_features[ratio_col_name] = (df_features[col_name] / df_features[top_neighborhood_pings_col_feat].replace(0, np.nan)).fillna(0)\n",
    "                    # else columns remain 0 as initialized\n",
    "            else:\n",
    "                print(\"Uyarı: Hiçbir cihaz için en sık ziyaret edilen mahallede zaman dilimi bazlı ping bulunamadı.\")\n",
    "        else:\n",
    "            print(\"Uyarı: Mahalle ping sayıları hesaplanamadı (neighborhood_ping_counts_for_top boş).\")\n",
    "    else:\n",
    "        print(f\"Uyarı: Mahalle sütunu ('{cleaned_neighborhood_col}') bulunamadığı veya boş olduğu için en sık mahalle zaman özellikleri hesaplanamadı.\")\n",
    "\n",
    "    print(\"\\nÖzellik çıkarımı tamamlandı.\")\n",
    "else:\n",
    "    print(\"Giriş verisi (df_results) yüklenemediği veya gerekli sütunlar eksik olduğu için özellik çıkarımı yapılamadı.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Çıkarılan Özellikleri İnceleme ve Kaydetme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_features' in locals() and not df_features.empty:\n",
    "    print(\"\\nÇıkarılan Özellikler DataFrame'i (df_features) - İlk 5 satır:\")\n",
    "    pd.set_option('display.max_columns', None) \n",
    "    print(df_features.head())\n",
    "    pd.reset_option('display.max_columns') \n",
    "    print(f\"\\nÖzellik DataFrame Boyutu: {df_features.shape}\")\n",
    "    print(\"\\nÖzellik Sütunları:\")\n",
    "    for col in df_features.columns:\n",
    "        print(col)\n",
    "    \n",
    "    try:\n",
    "        df_features.to_csv(output_file_path, index=True) \n",
    "        print(f\"\\nÖzellikler başarıyla '{output_file_path}' dosyasına kaydedildi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"HATA: Özellikler kaydedilirken bir sorun oluştu: {e}\")\n",
    "else:\n",
    "    print(\"\\nÖzellik DataFrame'i (df_features) oluşturulamadı veya boş.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
