{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Venue Visit Analysis\n",
    "\n",
    "This notebook performs an analysis to identify which venues from `filtered_data.csv` were visited by individuals based on mobility pings from `MobilityDataMay2024.parquet`.\n",
    "\n",
    "**Steps:**\n",
    "1. Load mobility data and venue data.\n",
    "2. Convert both to GeoDataFrames.\n",
    "3. Create a 50-meter buffer around each venue.\n",
    "4. Perform a spatial join to find mobility pings falling within these venue buffers.\n",
    "5. Save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "from shapely.geometry import Point\n",
    "import pyarrow # Required for parquet\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive (if running in Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted.\")\n",
    "    # Define base path for files on Google Drive\n",
    "    # IMPORTANT: Adjust this path if your files are in a subfolder of MyDrive\n",
    "    google_drive_base_path = '/content/drive/MyDrive/'\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, or google.colab module not found. Assuming files are local.\")\n",
    "    # Define a base path for local files (current directory)\n",
    "    google_drive_base_path = './' # Current directory if not in Colab\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")\n",
    "    google_drive_base_path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Function for Coordinate Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_coordinates(coord_series):\n",
    "    \"\"\"Cleans coordinate strings by replacing commas with periods and converting to numeric.\"\"\"\n",
    "    return pd.to_numeric(coord_series.astype(str).str.replace(',', '.', regex=False), errors='coerce')\n",
    "\n",
    "print(\"Helper function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Mobility Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading mobility data (MobilityDataMay2024.parquet)...\")\n",
    "# Construct path using the google_drive_base_path variable\n",
    "mobility_data_filename = 'MobilityDataMay2024.parquet'\n",
    "mobility_data_path = os.path.join(google_drive_base_path, mobility_data_filename)\n",
    "df_mobility = None\n",
    "try:\n",
    "    # Load the entire parquet file first\n",
    "    df_mobility_full = pd.read_parquet(mobility_data_path)\n",
    "    print(f\"Full mobility data loaded. Shape: {df_mobility_full.shape}\")\n",
    "    \n",
    "    # Take the first 1 million rows\n",
    "    num_rows_to_sample = 1000000\n",
    "    if len(df_mobility_full) > num_rows_to_sample:\n",
    "        df_mobility = df_mobility_full.head(num_rows_to_sample)\n",
    "        print(f\"Using the first {num_rows_to_sample} rows of mobility data. New shape: {df_mobility.shape}\")\n",
    "    else:\n",
    "        df_mobility = df_mobility_full\n",
    "        print(f\"Full mobility data has {len(df_mobility_full)} rows (less than or equal to 1 million), using all of it. Shape: {df_mobility.shape}\")\n",
    "    del df_mobility_full # Free up memory\n",
    "\n",
    "    # Basic validation\n",
    "    if not all(col in df_mobility.columns for col in ['latitude', 'longitude', 'device_aid', 'timestamp']):\n",
    "        print(\"Error: Mobility data is missing one or more required columns: 'latitude', 'longitude', 'device_aid', 'timestamp'.\")\n",
    "        df_mobility = None # Invalidate df_mobility\n",
    "    else:\n",
    "        print(\"Required columns found in mobility data.\")\n",
    "        print(df_mobility.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Mobility data file not found at {mobility_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading mobility data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Mobility GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_mobility = None\n",
    "if df_mobility is not None:\n",
    "    print(\"Creating mobility GeoDataFrame...\")\n",
    "    try:\n",
    "        gdf_mobility = geopandas.GeoDataFrame(\n",
    "            df_mobility,\n",
    "            geometry=geopandas.points_from_xy(df_mobility.longitude, df_mobility.latitude),\n",
    "            crs=\"EPSG:4326\"  # WGS84\n",
    "        )\n",
    "        print(f\"Mobility GeoDataFrame created. Shape: {gdf_mobility.shape}, CRS: {gdf_mobility.crs}\")\n",
    "        print(gdf_mobility.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating mobility GeoDataFrame: {e}\")\n",
    "else:\n",
    "    print(\"Skipping mobility GeoDataFrame creation as df_mobility was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Venue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading venue data (filtered_data.csv)...\")\n",
    "# Construct path using the google_drive_base_path variable\n",
    "venue_data_filename = 'filtered_data.csv'\n",
    "venue_data_path = os.path.join(google_drive_base_path, venue_data_filename)\n",
    "df_venues = None\n",
    "try:\n",
    "    df_venues = pd.read_csv(venue_data_path, sep=';')\n",
    "    print(f\"Venue data loaded. Shape: {df_venues.shape}\")\n",
    "    # Basic validation\n",
    "    if not all(col in df_venues.columns for col in ['lat', 'lng', 'MusteriKodu']):\n",
    "        print(\"Error: Venue data is missing one or more required columns: 'lat', 'lng', 'MusteriKodu'.\")\n",
    "        df_venues = None # Invalidate df_venues\n",
    "    else:\n",
    "        print(\"Required columns found in venue data.\")\n",
    "        print(df_venues.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Venue data file not found at {venue_data_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading venue data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean Venue Coordinates and Create Venue GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_venues = None\n",
    "if df_venues is not None:\n",
    "    print(\"Cleaning venue coordinates and creating venue GeoDataFrame...\")\n",
    "    try:\n",
    "        df_venues['lat_cleaned'] = clean_coordinates(df_venues['lat'])\n",
    "        df_venues['lng_cleaned'] = clean_coordinates(df_venues['lng'])\n",
    "        \n",
    "        # Drop rows with invalid coordinates\n",
    "        original_venue_count = len(df_venues)\n",
    "        df_venues.dropna(subset=['lat_cleaned', 'lng_cleaned'], inplace=True)\n",
    "        print(f\"Dropped {original_venue_count - len(df_venues)} venues due to invalid coordinates.\")\n",
    "\n",
    "        if df_venues.empty:\n",
    "            print(\"Error: No valid venue coordinates after cleaning.\")\n",
    "        else:\n",
    "            gdf_venues = geopandas.GeoDataFrame(\n",
    "                df_venues,\n",
    "                geometry=geopandas.points_from_xy(df_venues.lng_cleaned, df_venues.lat_cleaned),\n",
    "                crs=\"EPSG:4326\"  # WGS84\n",
    "            )\n",
    "            print(f\"Venue GeoDataFrame created. Shape: {gdf_venues.shape}, CRS: {gdf_venues.crs}\")\n",
    "            print(gdf_venues.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating venue GeoDataFrame: {e}\")\n",
    "else:\n",
    "    print(\"Skipping venue GeoDataFrame creation as df_venues was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spatial Analysis: Buffering and Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_visits = None\n",
    "if gdf_venues is not None and gdf_mobility is not None:\n",
    "    # Target CRS for buffering and spatial join (UTM Zone 36N, suitable for Istanbul/Western Turkey)\n",
    "    # If data covers a wider area, a more dynamic UTM zone selection or a different projected CRS might be needed.\n",
    "    projected_crs = \"EPSG:32636\" \n",
    "    buffer_radius_meters = 50\n",
    "\n",
    "    print(f\"Projecting venue data to {projected_crs} for buffering...\")\n",
    "    try:\n",
    "        gdf_venues_projected = gdf_venues.to_crs(projected_crs)\n",
    "        print(f\"Venue data projected. CRS: {gdf_venues_projected.crs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error projecting venue data: {e}\")\n",
    "        gdf_venues_projected = None\n",
    "\n",
    "    gdf_venue_buffers = None\n",
    "    if gdf_venues_projected is not None:\n",
    "        print(f\"Creating {buffer_radius_meters}m buffers around venues...\")\n",
    "        try:\n",
    "            # Ensure the geometry column is active and valid before buffering\n",
    "            if not gdf_venues_projected.geometry.is_valid.all():\n",
    "                print(\"Warning: Some venue geometries are invalid. Attempting to fix...\")\n",
    "                # A common trick to fix invalid geometries; may not always work perfectly.\n",
    "                gdf_venues_projected.geometry = gdf_venues_projected.geometry.buffer(0) \n",
    "                if not gdf_venues_projected.geometry.is_valid.all():\n",
    "                     print(\"Error: Could not fix all invalid venue geometries. Proceeding with potentially problematic data.\")\n",
    "            \n",
    "            gdf_venue_buffers = gdf_venues_projected.copy()\n",
    "            gdf_venue_buffers['geometry'] = gdf_venues_projected.geometry.buffer(buffer_radius_meters)\n",
    "            print(f\"Venue buffers created. Shape: {gdf_venue_buffers.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating venue buffers: {e}\")\n",
    "            gdf_venue_buffers = None\n",
    "\n",
    "    gdf_mobility_projected = None\n",
    "    if gdf_venue_buffers is not None: # Proceed only if buffers were created\n",
    "        print(f\"Projecting mobility data to {projected_crs} for spatial join...\")\n",
    "        try:\n",
    "            gdf_mobility_projected = gdf_mobility.to_crs(projected_crs)\n",
    "            print(f\"Mobility data projected. CRS: {gdf_mobility_projected.crs}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error projecting mobility data: {e}\")\n",
    "            gdf_mobility_projected = None\n",
    "\n",
    "    if gdf_mobility_projected is not None and gdf_venue_buffers is not None:\n",
    "        print(\"Performing spatial join (mobile pings within venue buffers)...\")\n",
    "        try:\n",
    "            # Use all columns from gdf_venue_buffers for the join to retain all venue information\n",
    "            gdf_venue_buffers_for_join = gdf_venue_buffers.copy()\n",
    "            print(f\"Columns in gdf_venue_buffers_for_join before sjoin: {gdf_venue_buffers_for_join.columns.tolist()}\")\n",
    "            \n",
    "            # Perform the spatial join\n",
    "            # 'predicate=\"within\"' means mobility points must be within venue buffers\n",
    "            gdf_visits = geopandas.sjoin(gdf_mobility_projected, gdf_venue_buffers_for_join, how='inner', predicate='within')\n",
    "            print(f\"Spatial join completed. Number of potential visit pings: {gdf_visits.shape[0]}\")\n",
    "            if gdf_visits.empty:\n",
    "                print(\"No visits found after spatial join.\")\n",
    "            else:\n",
    "                print(\"Sample of joined visit data (first 5 rows):\")\n",
    "                print(gdf_visits.head())\n",
    "        except Exception as e:\n",
    "            print(f\"Error during spatial join: {e}\")\n",
    "            gdf_visits = None\n",
    "else:\n",
    "    print(\"Skipping spatial analysis as one or both GeoDataFrames (mobility, venues) are missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process Results and Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gdf_visits is not None and not gdf_visits.empty:\n",
    "    print(\"Extracting relevant columns for the final output...\")\n",
    "    \n",
    "    # gdf_visits index comes from gdf_mobility_projected, which in turn comes from gdf_mobility (and df_mobility)\n",
    "    # We can use this index to retrieve original lat/lon from the initial df_mobility\n",
    "    df_visits_output = gdf_visits.copy()\n",
    "    \n",
    "    # Check if the index of df_visits_output is present in df_mobility.index\n",
    "    # This assumes that the index was preserved through GeoDataFrame conversions and projections.\n",
    "    # If sampling or other index-altering operations were done on df_mobility before creating gdf_mobility,\n",
    "    # this direct index lookup might fail or be incorrect.\n",
    "    # For this script, we assume the index is consistent.\n",
    "    try:\n",
    "        df_visits_output['original_latitude'] = df_mobility.loc[df_visits_output.index, 'latitude'].values\n",
    "        df_visits_output['original_longitude'] = df_mobility.loc[df_visits_output.index, 'longitude'].values\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError while trying to map original lat/lon: {e}. Original coordinates might be missing.\")\n",
    "        df_visits_output['original_latitude'] = pd.NA\n",
    "        df_visits_output['original_longitude'] = pd.NA\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while mapping original lat/lon: {e}\")\n",
    "        df_visits_output['original_latitude'] = pd.NA\n",
    "        df_visits_output['original_longitude'] = pd.NA\n",
    "\n",
    "    # Columns from mobility data (device_aid, timestamp, and the original lat/lon of the ping)\n",
    "    mobility_cols_to_keep = ['device_aid', 'timestamp', 'original_latitude', 'original_longitude']\n",
    "\n",
    "    # Original columns from df_venues (loaded from filtered_data.csv)\n",
    "    # Exclude helper columns we added like 'lat_cleaned', 'lng_cleaned', and the 'geometry' column itself from gdf_venues\n",
    "    # df_venues is the DataFrame loaded from filtered_data.csv and subsequently used to create gdf_venues\n",
    "    original_venue_column_names = [col for col in df_venues.columns if col not in ['lat', 'lng', 'lat_cleaned', 'lng_cleaned', 'geometry']]\n",
    "    \n",
    "    processed_venue_columns_for_output = []\n",
    "    # df_visits_output contains columns from gdf_mobility_projected and gdf_venue_buffers_for_join (which has all original venue columns)\n",
    "    # We need to handle potential suffixes added by sjoin, typically '_right' for columns from the right GeoDataFrame (gdf_venue_buffers_for_join)\n",
    "    for venue_col_original_name in original_venue_column_names:\n",
    "        # Check if the original name exists (it might if it wasn't duplicated in gdf_mobility_projected)\n",
    "        if venue_col_original_name in df_visits_output.columns:\n",
    "            processed_venue_columns_for_output.append(venue_col_original_name)\n",
    "        # Check if the suffixed version exists\n",
    "        elif f\"{venue_col_original_name}_right\" in df_visits_output.columns:\n",
    "            # Rename the suffixed column to its original name for cleaner output\n",
    "            df_visits_output.rename(columns={f\"{venue_col_original_name}_right\": venue_col_original_name}, inplace=True)\n",
    "            processed_venue_columns_for_output.append(venue_col_original_name)\n",
    "        # If neither exists, it means the column was not in the sjoin result (e.g. if it was all NaNs and dropped, or not selected for join - though we select all now)\n",
    "\n",
    "    # Combine mobility columns and all successfully processed (and renamed) venue columns\n",
    "    # Ensure 'geometry' from the venue side (if present and suffixed) is not included unless explicitly desired\n",
    "    # Also, 'index_right' is an sjoin artifact we usually don't need in the final output.\n",
    "    # The 'geometry' column in df_visits_output is from the mobility pings (left side of sjoin).\n",
    "    final_output_columns_list = mobility_cols_to_keep + processed_venue_columns_for_output\n",
    "    \n",
    "    # Ensure all selected columns actually exist in df_visits_output to prevent KeyErrors,\n",
    "    # and remove duplicates if any column name was accidentally in both lists or processed multiple times.\n",
    "    # Also, remove helper columns that might have slipped through from the venue side if they were named like original data columns.\n",
    "    columns_to_exclude_finally = ['geometry_right', 'index_right', 'lat_cleaned', 'lng_cleaned'] # geometry_right might appear if venue geometry was also named 'geometry'\n",
    "    \n",
    "    final_selected_columns = []\n",
    "    seen_columns = set()\n",
    "    for col in final_output_columns_list:\n",
    "        if col in df_visits_output.columns and col not in columns_to_exclude_finally and col not in seen_columns:\n",
    "            final_selected_columns.append(col)\n",
    "            seen_columns.add(col)\n",
    "            \n",
    "    print(f\"Final columns selected for output: {final_selected_columns}\")\n",
    "        \n",
    "    df_final_visits = df_visits_output[final_selected_columns]\n",
    "\n",
    "    output_filename = 'device_venue_visits.csv'\n",
    "    print(f\"Saving results to {output_filename}...\")\n",
    "    try:\n",
    "        df_final_visits.to_csv(output_filename, index=False, sep=';')\n",
    "        print(f\"Successfully saved results to {output_filename}. Shape: {df_final_visits.shape}\")\n",
    "        print(df_final_visits.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "else:\n",
    "    print(\"No visit data to save (either gdf_visits is None or empty).\")\n",
    "\n",
    "print(\"Venue visit analysis script finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Additional Device Features and Merge\n",
    "\n",
    "Load `device_venue_visits.csv` and `device_featuresv2.csv`, then merge them on `device_aid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading device_venue_visits.csv...\")\n",
    "visits_df = None\n",
    "try:\n",
    "    visits_df = pd.read_csv('device_venue_visits.csv', sep=';')\n",
    "    print(f\"device_venue_visits.csv loaded. Shape: {visits_df.shape}\")\n",
    "    print(visits_df.head())\n",
    "    if 'device_aid' not in visits_df.columns:\n",
    "        print(\"Error: 'device_aid' column not found in device_venue_visits.csv\")\n",
    "        visits_df = None\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: device_venue_visits.csv not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading device_venue_visits.csv: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading device_featuresv2.csv...\")\n",
    "features_df = None\n",
    "features_filename = 'device_featuresv2.csv'\n",
    "features_path = os.path.join(google_drive_base_path, features_filename)\n",
    "try:\n",
    "    features_df = pd.read_csv(features_path) # Assuming standard comma separator\n",
    "    print(f\"{features_filename} loaded from {features_path}. Shape: {features_df.shape}\")\n",
    "    \n",
    "    # If the first column is named 'Unnamed: 0' (pandas default for blank header), rename it to 'device_aid'\n",
    "    if not features_df.empty and features_df.columns[0] == 'Unnamed: 0':\n",
    "        features_df.rename(columns={'Unnamed: 0': 'device_aid'}, inplace=True)\n",
    "        print(\"Renamed first column from 'Unnamed: 0' to 'device_aid'.\")\n",
    "    \n",
    "    print(features_df.head())\n",
    "    \n",
    "    if 'device_aid' not in features_df.columns:\n",
    "        print(f\"Error: 'device_aid' column not found or not correctly named in {features_filename}. Please check CSV header.\")\n",
    "        features_df = None\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {features_filename} not found at {features_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {features_filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = None\n",
    "if visits_df is not None and features_df is not None:\n",
    "    print(\"Merging dataframes on 'device_aid'...\")\n",
    "    try:\n",
    "        # Ensure device_aid is of the same type if necessary, though pandas often handles this.\n",
    "        # For example, if one is int and other is string: \n",
    "        # visits_df['device_aid'] = visits_df['device_aid'].astype(str)\n",
    "        # features_df['device_aid'] = features_df['device_aid'].astype(str)\n",
    "        merged_df = pd.merge(visits_df, features_df, on='device_aid', how='inner')\n",
    "        print(f\"Merge successful. Shape of merged_df: {merged_df.shape}\")\n",
    "        print(\"Merged DataFrame head:\")\n",
    "        print(merged_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge: {e}\")\n",
    "else:\n",
    "    print(\"Skipping merge as one or both dataframes were not loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merged_df is not None and not merged_df.empty:\n",
    "    output_merged_filename = 'mobil_restaurant.csv'\n",
    "    print(f\"Saving merged data to {output_merged_filename}...\")\n",
    "    try:\n",
    "        merged_df.to_csv(output_merged_filename, index=False, sep=';')\n",
    "        print(f\"Successfully saved merged data to {output_merged_filename}. Shape: {merged_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving merged data: {e}\")\n",
    "else:\n",
    "    print(\"No merged data to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
